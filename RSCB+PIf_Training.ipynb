{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d692d4f61a09451b93898fde67b37033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_577619d7b7e6457e82280fc64f7e54ba",
              "IPY_MODEL_601b88697bcc4ba5861901d3c824b200",
              "IPY_MODEL_e129532baa854fd99d94096e11fae663"
            ],
            "layout": "IPY_MODEL_06fa5c0ede4e41c098d9b1fb06e620c1"
          }
        },
        "577619d7b7e6457e82280fc64f7e54ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4347f357ac41f68b8661c2004aa196",
            "placeholder": "​",
            "style": "IPY_MODEL_458258dddc4540d59c58294c400e9c5a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "601b88697bcc4ba5861901d3c824b200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e63eb10adc44488814822b1712ad75",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13d7fec6d72944f985f17e52e658ad69",
            "value": 2
          }
        },
        "e129532baa854fd99d94096e11fae663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66bc362d086042dfa93fd317b05f759b",
            "placeholder": "​",
            "style": "IPY_MODEL_6ba605e3083747238f9f4b54df70d44a",
            "value": " 2/2 [00:03&lt;00:00,  1.49s/it]"
          }
        },
        "06fa5c0ede4e41c098d9b1fb06e620c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f4347f357ac41f68b8661c2004aa196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "458258dddc4540d59c58294c400e9c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68e63eb10adc44488814822b1712ad75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13d7fec6d72944f985f17e52e658ad69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66bc362d086042dfa93fd317b05f759b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba605e3083747238f9f4b54df70d44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac5c7fce22a349daaa1d7f76c7aa002b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_533a0c97ded643c99d782b63d8057462",
              "IPY_MODEL_4c0d564c102a4977af3615dbfb2697e9",
              "IPY_MODEL_a0f192b7cc2349c787507f8f2b5916c8"
            ],
            "layout": "IPY_MODEL_a032035f90924119ae916fb070cbb168"
          }
        },
        "533a0c97ded643c99d782b63d8057462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff0456959fda46eea106cea499865cc2",
            "placeholder": "​",
            "style": "IPY_MODEL_8618bc4f8e4a44f3a4ebf5a64ce831e3",
            "value": "Map: 100%"
          }
        },
        "4c0d564c102a4977af3615dbfb2697e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_526172eaa2ba4644999608fc098fdb0c",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3645e72959b948e89d7262e3eb818d60",
            "value": 200
          }
        },
        "a0f192b7cc2349c787507f8f2b5916c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_912e45770df645b68edd1562bd29d2d8",
            "placeholder": "​",
            "style": "IPY_MODEL_7cce2b63300d40129857abdf3ee0e19b",
            "value": " 200/200 [00:00&lt;00:00, 713.55 examples/s]"
          }
        },
        "a032035f90924119ae916fb070cbb168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0456959fda46eea106cea499865cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8618bc4f8e4a44f3a4ebf5a64ce831e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "526172eaa2ba4644999608fc098fdb0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3645e72959b948e89d7262e3eb818d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "912e45770df645b68edd1562bd29d2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cce2b63300d40129857abdf3ee0e19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4b043312e2341d7b2267f4d96414d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9fb44d28a6e4d47bb89db3058862aa2",
              "IPY_MODEL_179679aca202453c9e4b4698ce23c129",
              "IPY_MODEL_d6fba254425f4f46904bcf268cb86cb5"
            ],
            "layout": "IPY_MODEL_8c68d0ea0fd941a0b0fa18921a83ef82"
          }
        },
        "f9fb44d28a6e4d47bb89db3058862aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8d4cd8a0f1141b6820631754bc03e08",
            "placeholder": "​",
            "style": "IPY_MODEL_1662cdacff254b0eb7b8761f7684cb3c",
            "value": "Map: 100%"
          }
        },
        "179679aca202453c9e4b4698ce23c129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a860c57965649449498f35312f70adf",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5f57485cda64ca5bcbe316187662f84",
            "value": 200
          }
        },
        "d6fba254425f4f46904bcf268cb86cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb9fdfbfac3c415d991994d05429ecd9",
            "placeholder": "​",
            "style": "IPY_MODEL_f826b9d9aa6c4b04917aa5d077b501e1",
            "value": " 200/200 [00:00&lt;00:00, 1634.05 examples/s]"
          }
        },
        "8c68d0ea0fd941a0b0fa18921a83ef82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d4cd8a0f1141b6820631754bc03e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1662cdacff254b0eb7b8761f7684cb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a860c57965649449498f35312f70adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f57485cda64ca5bcbe316187662f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb9fdfbfac3c415d991994d05429ecd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f826b9d9aa6c4b04917aa5d077b501e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5fa307608a04af28e9a14ba856f6a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6352da4ea984e139ff7bef8cdb22951",
              "IPY_MODEL_57984a4c7f7a4dd695e44438a4a03ac3",
              "IPY_MODEL_cf2714008f224369ad5ca48403a57516"
            ],
            "layout": "IPY_MODEL_7c98c8e174724e138ea1aed5baf281eb"
          }
        },
        "b6352da4ea984e139ff7bef8cdb22951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0a27730b9c46058ca0997ad2bfcdef",
            "placeholder": "​",
            "style": "IPY_MODEL_c86e8f3b58774ff5ab60a07d2a901472",
            "value": "model.safetensors: 100%"
          }
        },
        "57984a4c7f7a4dd695e44438a4a03ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3e1e5a962d348b0aafc34b05c7747b4",
            "max": 6425529112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31727d0f86124b4e84c1bc6152f909f3",
            "value": 6425529112
          }
        },
        "cf2714008f224369ad5ca48403a57516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c82257ece44b128c229132cd8517ee",
            "placeholder": "​",
            "style": "IPY_MODEL_98e1441d414840908635d4201e25e7ce",
            "value": " 6.43G/6.43G [02:34&lt;00:00, 39.6MB/s]"
          }
        },
        "7c98c8e174724e138ea1aed5baf281eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d0a27730b9c46058ca0997ad2bfcdef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86e8f3b58774ff5ab60a07d2a901472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3e1e5a962d348b0aafc34b05c7747b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31727d0f86124b4e84c1bc6152f909f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6c82257ece44b128c229132cd8517ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e1441d414840908635d4201e25e7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aacf7b2da1be443686d214817f6dc08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e394bcf28e94e5da38cae97e02ed5fe",
              "IPY_MODEL_a33e43e656684fa38374a75a914a2ad1",
              "IPY_MODEL_d98fe590dcfb4e00a3222efd121e5369"
            ],
            "layout": "IPY_MODEL_52fb1bf836174b20891bda9b010f5a80"
          }
        },
        "7e394bcf28e94e5da38cae97e02ed5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec62e55d078d4b3d9d430ec1d80d4fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_7e6d39f455294ecdb5561536ed110d27",
            "value": "generation_config.json: 100%"
          }
        },
        "a33e43e656684fa38374a75a914a2ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34e3da16f21c4a7cb3ba2ba40be98668",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_913ab0ba8bb8457bb5c4443ab88208bc",
            "value": 121
          }
        },
        "d98fe590dcfb4e00a3222efd121e5369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_728ddcdb109e46d9adc34cc1a406f5e6",
            "placeholder": "​",
            "style": "IPY_MODEL_73cb3b908d57427282bca430fedb6cd5",
            "value": " 121/121 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "52fb1bf836174b20891bda9b010f5a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec62e55d078d4b3d9d430ec1d80d4fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6d39f455294ecdb5561536ed110d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34e3da16f21c4a7cb3ba2ba40be98668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913ab0ba8bb8457bb5c4443ab88208bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "728ddcdb109e46d9adc34cc1a406f5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cb3b908d57427282bca430fedb6cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dcd7b7ee1ff42259b44076c7992b9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d3f7fb1679542b794e5609ce99899f4",
              "IPY_MODEL_14046e694f9543a3b626a36c581af24c",
              "IPY_MODEL_d8f8de58884a44c29ea10a985855524d"
            ],
            "layout": "IPY_MODEL_2c9e048242c74c9a9391ff01e6f90f14"
          }
        },
        "1d3f7fb1679542b794e5609ce99899f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a850c782f82498bbbd4c7d6c7c5184c",
            "placeholder": "​",
            "style": "IPY_MODEL_a2a143e20b324202a440c88a52e374c9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "14046e694f9543a3b626a36c581af24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1891556a846413bab4f29913275395e",
            "max": 50570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16422f7716354f82b6c992281e3900a5",
            "value": 50570
          }
        },
        "d8f8de58884a44c29ea10a985855524d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a974e065f9a4661a2702644f97765a9",
            "placeholder": "​",
            "style": "IPY_MODEL_b81d7552b17d405aaf03781cb3ecfb04",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 3.97MB/s]"
          }
        },
        "2c9e048242c74c9a9391ff01e6f90f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a850c782f82498bbbd4c7d6c7c5184c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a143e20b324202a440c88a52e374c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1891556a846413bab4f29913275395e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16422f7716354f82b6c992281e3900a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a974e065f9a4661a2702644f97765a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b81d7552b17d405aaf03781cb3ecfb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0781c9e221774965994b51d1c5bb862c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2206f186776e4c6e9c97c17e51a8ded2",
              "IPY_MODEL_dd8a335a713346c884caa132b22656b4",
              "IPY_MODEL_f15fc00779f243a1bc8b5af794f40c44"
            ],
            "layout": "IPY_MODEL_aef0e8d71514467ebcab0236bf036c6b"
          }
        },
        "2206f186776e4c6e9c97c17e51a8ded2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58b04428032e4b4497c39d820e9a7df4",
            "placeholder": "​",
            "style": "IPY_MODEL_d3b5756f2c5c4181a6c45124344818e0",
            "value": "tokenizer.json: 100%"
          }
        },
        "dd8a335a713346c884caa132b22656b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed40a4a39b2742ef91ee6c932c83d413",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50f032e46ef742a0adcc04570096145e",
            "value": 9085657
          }
        },
        "f15fc00779f243a1bc8b5af794f40c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10799e397d46458f83da4f60a098066d",
            "placeholder": "​",
            "style": "IPY_MODEL_7863ea2471b54c2a9574a8d1a86ae226",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 42.4MB/s]"
          }
        },
        "aef0e8d71514467ebcab0236bf036c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b04428032e4b4497c39d820e9a7df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b5756f2c5c4181a6c45124344818e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed40a4a39b2742ef91ee6c932c83d413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f032e46ef742a0adcc04570096145e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10799e397d46458f83da4f60a098066d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7863ea2471b54c2a9574a8d1a86ae226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfe51ccb7691422eb68b404fd2ddb1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9fbdb276f204aa0b200ea7997ea89b4",
              "IPY_MODEL_812ee87a2a5b4c4baa69a44ff213975e",
              "IPY_MODEL_260122f7c4cb48ca81593ab1b777c729"
            ],
            "layout": "IPY_MODEL_c76c3b0f617e4aa88855765f85c38849"
          }
        },
        "e9fbdb276f204aa0b200ea7997ea89b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f0be2f16cc4b1c8ed1ad9cca3c49d7",
            "placeholder": "​",
            "style": "IPY_MODEL_7e1cf2ed6f7c4e8b97b6089cf511ed82",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "812ee87a2a5b4c4baa69a44ff213975e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4198c8a1bbd44eaeae2e14f0d93e95c4",
            "max": 459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6aa6dade958c4c0682232f44550d45cb",
            "value": 459
          }
        },
        "260122f7c4cb48ca81593ab1b777c729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e04ebf54b85a4da08cf093e06476da74",
            "placeholder": "​",
            "style": "IPY_MODEL_09b6d50727374f7da77d76619138b3fe",
            "value": " 459/459 [00:00&lt;00:00, 35.5kB/s]"
          }
        },
        "c76c3b0f617e4aa88855765f85c38849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f0be2f16cc4b1c8ed1ad9cca3c49d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1cf2ed6f7c4e8b97b6089cf511ed82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4198c8a1bbd44eaeae2e14f0d93e95c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa6dade958c4c0682232f44550d45cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e04ebf54b85a4da08cf093e06476da74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b6d50727374f7da77d76619138b3fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e56dadc3e86b49c79fc16ac7d215be70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08ca1ef707e24e3eac4fc98597fd7e7e",
              "IPY_MODEL_be584c95da6d45c1820774e28b534c74",
              "IPY_MODEL_d1558a734a68436f89d2153141d87620"
            ],
            "layout": "IPY_MODEL_f3e5305828044bae896f93044a6e12d1"
          }
        },
        "08ca1ef707e24e3eac4fc98597fd7e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d23252ff2d94378a2bd4d127261ec16",
            "placeholder": "​",
            "style": "IPY_MODEL_dc87470c182a4376836100c228453bea",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "be584c95da6d45c1820774e28b534c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a391a9d1bd04d15909d7d722d22bb36",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b22cdf0dc40420da873b7afe817c150",
            "value": 2
          }
        },
        "d1558a734a68436f89d2153141d87620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a05dd452e1b45af9070cf657e19a363",
            "placeholder": "​",
            "style": "IPY_MODEL_7f66787c0b054927aa1918a9246b0358",
            "value": " 2/2 [00:55&lt;00:00, 24.85s/it]"
          }
        },
        "f3e5305828044bae896f93044a6e12d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d23252ff2d94378a2bd4d127261ec16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc87470c182a4376836100c228453bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a391a9d1bd04d15909d7d722d22bb36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b22cdf0dc40420da873b7afe817c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a05dd452e1b45af9070cf657e19a363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f66787c0b054927aa1918a9246b0358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "325a09a041ea44029a3368d3492b9cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc69dc80d16d4b12b83a7cadf9e98050",
              "IPY_MODEL_43d7e807b14d445f9a83089189931eb6",
              "IPY_MODEL_78e54325eec64a61b7f67786727aef60"
            ],
            "layout": "IPY_MODEL_4a23cfc458ca48ab97bbf71d98d236a5"
          }
        },
        "bc69dc80d16d4b12b83a7cadf9e98050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26fda3197a294f9b9d4ffc2c704abe17",
            "placeholder": "​",
            "style": "IPY_MODEL_5706070ac9bc4c17beec02cb5fcd647f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "43d7e807b14d445f9a83089189931eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bd7fec617342f5a4516805cede0b9e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0816e88c969d42c98129dcb60495cc01",
            "value": 2
          }
        },
        "78e54325eec64a61b7f67786727aef60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fe201e989342479382a352e026a19f",
            "placeholder": "​",
            "style": "IPY_MODEL_37e6bd0e5d964e1da98ac7583246b974",
            "value": " 2/2 [00:06&lt;00:00,  2.81s/it]"
          }
        },
        "4a23cfc458ca48ab97bbf71d98d236a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26fda3197a294f9b9d4ffc2c704abe17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5706070ac9bc4c17beec02cb5fcd647f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01bd7fec617342f5a4516805cede0b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0816e88c969d42c98129dcb60495cc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23fe201e989342479382a352e026a19f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e6bd0e5d964e1da98ac7583246b974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPAs56C66sKP",
        "outputId": "43149b2e-5475-46a4-c563-5329dc13f618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.1.1)\n",
            "Collecting datasets>=2.21.0 (from trl)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.46.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (4.66.6)\n",
            "Collecting xxhash (from datasets>=2.21.0->trl)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.20.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 trl-0.12.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_4stmBS6ddi",
        "outputId": "34beb0b8-0f01-4ae7-a706-1ade5096ab52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from dataclasses import dataclass\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union,Any, Dict, List"
      ],
      "metadata": {
        "id": "mVd4oCalvhsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QSsD4s_xWlq5",
        "outputId": "928fc483-8cfa-4959-9fcf-d27edc54b9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2024.12.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2024.11.8 (from unsloth)\n",
            "  Downloading unsloth_zoo-2024.12.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.46.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.1.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.12.1)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
            "Collecting protobuf<4.0.0 (from unsloth)\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.26.2)\n",
            "Collecting hf_transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (0.20.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2024.11.8->unsloth)\n",
            "  Downloading cut_cross_entropy-24.12.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth) (11.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Downloading unsloth-2024.12.2-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2024.12.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.2-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-24.12.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: triton, shtab, protobuf, hf_transfer, xformers, tyro, cut_cross_entropy, bitsandbytes, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.44.1 cut_cross_entropy-24.12.1 hf_transfer-0.1.8 protobuf-3.20.3 shtab-1.7.1 triton-3.1.0 tyro-0.9.2 unsloth-2024.12.2 unsloth_zoo-2024.12.1 xformers-0.0.28.post3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "693ebdd46e6a4c9489a8a667f73064e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: unsloth 2024.12.2\n",
            "Uninstalling unsloth-2024.12.2:\n",
            "  Successfully uninstalled unsloth-2024.12.2\n",
            "Collecting git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-5_32m3zh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-5_32m3zh\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 9dc399a6b6625ee40835c5eab361426d3c5d4abb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.12.2-py3-none-any.whl size=167667 sha256=b6227f4bf123c3a394a7de116952e613453e5403019159b6b5bb5062ed432bc5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3h1cz4mc/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2024.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tTzu3aZWxcQ",
        "outputId": "0c19853b-e206-4941-ecae-ee374e8477aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBlyVPO7W3ra",
        "outputId": "eeacdea4-0560-4df1-c079-bc7feb2a285d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"\")"
      ],
      "metadata": {
        "id": "ROYGyufdYFMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/train_mcqa.csv')"
      ],
      "metadata": {
        "id": "mGCYltKp7RoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(results, data):\n",
        "    correct_predictions = 0\n",
        "    total_questions = 0\n",
        "\n",
        "    for result, entry in zip(results, data[\"data\"]):\n",
        "        correct_option = entry.get(\"correct_option\")\n",
        "        if not correct_option:\n",
        "            continue  # Skip if the correct option is not available\n",
        "\n",
        "        total_questions += 1  # Increment total questions for every entry\n",
        "\n",
        "        # Check if predicted_options is non-empty\n",
        "        if result[\"predicted_options\"]:\n",
        "            # Get the top 1 predicted option\n",
        "            predicted_option = result[\"predicted_options\"][0]\n",
        "\n",
        "            if predicted_option == correct_option:\n",
        "                correct_predictions += 1\n",
        "        else:\n",
        "            # Count empty predictions as incorrect\n",
        "            print(f\"No prediction for question ID: {entry['id']}\")\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions / total_questions if total_questions > 0 else 0.0\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Dw4TEdz_KR4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_map_at_k(results, data, k=3):\n",
        "    average_precisions = []\n",
        "\n",
        "    for result, entry in zip(results, data[\"data\"]):\n",
        "        correct_option = entry.get(\"correct_option\")\n",
        "        if not correct_option:\n",
        "            continue  # Skip if the correct option is not available\n",
        "\n",
        "        predicted_options = result[\"predicted_options\"]\n",
        "        try:\n",
        "            rank = predicted_options.index(correct_option) + 1\n",
        "            average_precision = 1.0 / rank\n",
        "        except ValueError:\n",
        "            average_precision = 0.0  # Correct option not in top k\n",
        "\n",
        "        average_precisions.append(average_precision)\n",
        "\n",
        "    # Compute MAP@k\n",
        "    map_at_k = sum(average_precisions) / len(average_precisions) if average_precisions else 0.0\n",
        "    return map_at_k"
      ],
      "metadata": {
        "id": "NBDMiL3PINhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_correct_answer_to_target_option(entry, target_option_label):\n",
        "    option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "    options = entry[\"options\"]\n",
        "    correct_answer = entry[\"correct_option\"]\n",
        "\n",
        "    # Ensure the correct answer exists in options\n",
        "    if correct_answer not in options:\n",
        "        return None  # Skip if the correct answer is not found\n",
        "\n",
        "    # Create a new options dictionary with the correct answer moved\n",
        "    new_options = {}\n",
        "    for label in option_labels:\n",
        "        if label == target_option_label:\n",
        "            new_options[label] = options[correct_answer]\n",
        "        elif label == correct_answer:\n",
        "            new_options[label] = options[target_option_label] if target_option_label in options else ''\n",
        "        else:\n",
        "            new_options[label] = options.get(label, '')\n",
        "\n",
        "    # Update the correct answer label\n",
        "    new_entry = entry.copy()\n",
        "    new_entry[\"options\"] = new_options\n",
        "    new_entry[\"correct_option\"] = target_option_label\n",
        "    return new_entry\n",
        "def transform_data(data, target_option_label):\n",
        "    transformed_data = {\"data\": []}\n",
        "    for entry in data[\"data\"]:\n",
        "        new_entry = move_correct_answer_to_target_option(entry, target_option_label)\n",
        "        if new_entry is not None:\n",
        "            transformed_data[\"data\"].append(new_entry)\n",
        "    return transformed_data\n",
        "\n",
        "def calculate_mu_bias_with_model(model, tokenizer, data):\n",
        "    option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "    # Compute Acc0 using your inference results\n",
        "    Acc_0 = compute_accuracy(inference_result, data)\n",
        "    print(f\"Original Accuracy (Acc0): {Acc_0}\")\n",
        "\n",
        "    Acc_list = []\n",
        "    for target_option_label in option_labels:\n",
        "        # Move the correct answers to the target option for all questions\n",
        "        data_moved = {\"data\": []}\n",
        "        for entry in data[\"data\"]:\n",
        "            new_entry = move_correct_answer_to_target_option(entry, target_option_label)\n",
        "            if new_entry is not None:\n",
        "                data_moved[\"data\"].append(new_entry)\n",
        "\n",
        "        # Re-run inference on the modified data\n",
        "        results_i = make_inference_on_mcqa_batch(model, tokenizer, data_moved)\n",
        "\n",
        "        # Compute accuracy after the answer-moving attack\n",
        "        Acc_i = compute_accuracy(results_i, data_moved)\n",
        "        Acc_list.append(Acc_i)\n",
        "        print(f\"Accuracy after moving correct answers to option '{target_option_label}' (Acc_i): {Acc_i}\")\n",
        "\n",
        "    # Calculate μ_bias\n",
        "    Acc_diffs = [abs(Acc_i - Acc_0) for Acc_i in Acc_list]\n",
        "    mu_bias = sum(Acc_diffs) / len(option_labels)\n",
        "    print(f\"μ_bias: {mu_bias}\")\n",
        "    return mu_bias"
      ],
      "metadata": {
        "id": "pzKdQSohcJd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading model from HuggingFace"
      ],
      "metadata": {
        "id": "qgyQ_zpB9jyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d692d4f61a09451b93898fde67b37033",
            "577619d7b7e6457e82280fc64f7e54ba",
            "601b88697bcc4ba5861901d3c824b200",
            "e129532baa854fd99d94096e11fae663",
            "06fa5c0ede4e41c098d9b1fb06e620c1",
            "2f4347f357ac41f68b8661c2004aa196",
            "458258dddc4540d59c58294c400e9c5a",
            "68e63eb10adc44488814822b1712ad75",
            "13d7fec6d72944f985f17e52e658ad69",
            "66bc362d086042dfa93fd317b05f759b",
            "6ba605e3083747238f9f4b54df70d44a"
          ]
        },
        "id": "DUnhpW33-YYE",
        "outputId": "4f652246-42d0-4877-d14f-c3f6cca1703b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d692d4f61a09451b93898fde67b37033"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the JSON data (modify path to where your file is)\n",
        "json_file_path = '/content/drive/MyDrive/formatted_training_data.json'  # Update path for Colab\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "-iNkQ4smuzDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_on_mcqa_mcp_generate2(model, tokenizer, data, k=1):\n",
        "    results = []\n",
        "\n",
        "    # Ensure the model is in evaluation mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "    for entry in data[\"data\"]:\n",
        "        prompt = entry[\"prompt\"]\n",
        "        question = entry[\"question\"]\n",
        "        options = entry[\"options\"]\n",
        "\n",
        "        # Construct the input prompt including all options\n",
        "        input_prompt = f\"{prompt}\\n\\nQuestion: {question}\\n\\nOptions:\\n\"\n",
        "        available_option_labels = []\n",
        "        for label in option_labels:\n",
        "            if label in options and options[label]:\n",
        "                option_text = options[label]\n",
        "                input_prompt += f\"{label}) {option_text}\\n\"\n",
        "                available_option_labels.append(label)\n",
        "\n",
        "        input_prompt += \"\\nAnswer:\"\n",
        "\n",
        "        # Tokenize input prompt without truncation\n",
        "        inputs = tokenizer(\n",
        "            input_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=False,\n",
        "            max_length=1024\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        # Generate output sequences using beam search\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                num_beams=5,             # Number of beams for beam search\n",
        "                max_new_tokens=50,       # Maximum number of tokens to generate\n",
        "                num_return_sequences=min(k, 5),  # Return up to k sequences\n",
        "                early_stopping=True,\n",
        "                do_sample=False          # Disable sampling to use beam search\n",
        "            )\n",
        "\n",
        "        # Decode the generated sequences and extract predicted labels\n",
        "        predicted_labels = []\n",
        "        for output in outputs:\n",
        "            # Get the generated tokens after the input prompt\n",
        "            generated_tokens = output[input_ids.shape[1]:]\n",
        "            generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "            # Extract the predicted label (assuming it's the first non-empty token)\n",
        "            predicted_label = generated_text.split()[0]\n",
        "\n",
        "            if predicted_label in available_option_labels and predicted_label not in predicted_labels:\n",
        "                predicted_labels.append(predicted_label)\n",
        "                if len(predicted_labels) >= k:\n",
        "                    break  # We have enough predictions\n",
        "\n",
        "        # If no valid labels are predicted, you can handle it accordingly\n",
        "        if not predicted_labels:\n",
        "            result = {\n",
        "                \"id\": entry[\"id\"],\n",
        "                \"question\": question,\n",
        "                \"predicted_options\": []\n",
        "            }\n",
        "        else:\n",
        "            result = {\n",
        "                \"id\": entry[\"id\"],\n",
        "                \"question\": question,\n",
        "                \"predicted_options\": predicted_labels  # List of top k predicted labels\n",
        "            }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cI80dzJT2GEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_on_mcqa_mcp_generate(model, tokenizer, data, k=1):\n",
        "    results = []\n",
        "\n",
        "    # Ensure the model is in evaluation mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "    for entry in data[\"data\"]:\n",
        "        prompt = entry[\"prompt\"]\n",
        "        question = entry[\"question\"]\n",
        "        options = entry[\"options\"]\n",
        "\n",
        "        input_prompt = f\"{prompt}\\n\\nQuestion: {question}\\n\\nOptions:\\n\"\n",
        "        available_option_labels = []\n",
        "        for label in option_labels:\n",
        "            if label in options and options[label]:\n",
        "                option_text = options[label]\n",
        "                input_prompt += f\"{label}) {option_text}\\n\"\n",
        "                available_option_labels.append(label)\n",
        "\n",
        "        input_prompt += \"\\nAnswer:\"\n",
        "\n",
        "        # Tokenize input prompt without truncation\n",
        "        inputs = tokenizer(\n",
        "            input_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=False\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        # Find the index after 'Answer:' in the input prompt\n",
        "        answer_str_index = input_prompt.find('Answer:')\n",
        "        if answer_str_index == -1:\n",
        "            print(f\"'Answer:' not found in the prompt for question ID {entry['id']}\")\n",
        "            result = {\n",
        "                \"id\": entry[\"id\"],\n",
        "                \"question\": question,\n",
        "                \"predicted_options\": []\n",
        "            }\n",
        "            results.append(result)\n",
        "            continue\n",
        "\n",
        "        # Tokenize the prompt up to 'Answer:' to find the position\n",
        "        partial_prompt = input_prompt[:answer_str_index + len('Answer:')]\n",
        "        partial_inputs = tokenizer(\n",
        "            partial_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=False\n",
        "        ).to(device)\n",
        "        answer_end_index = partial_inputs['input_ids'].shape[1] - 1  # Position after 'Answer:'\n",
        "\n",
        "        # Ensure the index is within bounds\n",
        "        if answer_end_index >= inputs['input_ids'].shape[1]:\n",
        "            print(f\"Next token index {answer_end_index} out of bounds for question ID {entry['id']}\")\n",
        "            result = {\n",
        "                \"id\": entry[\"id\"],\n",
        "                \"question\": question,\n",
        "                \"predicted_options\": []\n",
        "            }\n",
        "            results.append(result)\n",
        "            continue\n",
        "\n",
        "        # Get the logits for the next token after 'Answer:'\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Shape: [batch_size, seq_length, vocab_size]\n",
        "\n",
        "        next_token_logits = logits[0, answer_end_index, :]  # Shape: [vocab_size]\n",
        "\n",
        "        # Map labels to their token IDs\n",
        "        label_token_ids = []\n",
        "        for label in available_option_labels:\n",
        "            label_token_id = tokenizer.encode(label, add_special_tokens=False)\n",
        "            if len(label_token_id) > 0:\n",
        "                label_token_ids.append((label, label_token_id))\n",
        "            else:\n",
        "                print(f\"Label '{label}' tokenization failed for question ID {entry['id']}\")\n",
        "\n",
        "        if not label_token_ids:\n",
        "            print(f\"No valid labels for question ID {entry['id']}\")\n",
        "            result = {\n",
        "                \"id\": entry[\"id\"],\n",
        "                \"question\": question,\n",
        "                \"predicted_options\": []\n",
        "            }\n",
        "            results.append(result)\n",
        "            continue\n",
        "\n",
        "        # Extract logits for the label tokens\n",
        "        label_logits = []\n",
        "        for label, token_ids in label_token_ids:\n",
        "            logit = sum([next_token_logits[tid].item() for tid in token_ids])\n",
        "            label_logits.append(logit)\n",
        "        label_logits = torch.tensor(label_logits)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        label_probs = torch.softmax(label_logits, dim=0)\n",
        "\n",
        "        # Get the indices of the top k probabilities\n",
        "        top_k = min(k, len(label_probs))  # Ensure k does not exceed available options\n",
        "        topk_probs, topk_indices = torch.topk(label_probs, top_k)\n",
        "\n",
        "        # Get the corresponding labels\n",
        "        predicted_labels = [label_token_ids[idx][0] for idx in topk_indices]\n",
        "\n",
        "        result = {\n",
        "            \"id\": entry[\"id\"],\n",
        "            \"question\": question,\n",
        "            \"predicted_options\": predicted_labels  # List of top k predicted labels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "qZX0ZX1OyGaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_on_mcqa_cloze(model, tokenizer, data, k):\n",
        "    results = []\n",
        "\n",
        "    # Ensure the model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Move the model to the appropriate device\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "    # Since the dataset is small, process each question individually\n",
        "    for entry in data[\"data\"]:\n",
        "        prompt = entry[\"prompt\"]\n",
        "        question = entry[\"question\"]\n",
        "        options = entry[\"options\"]\n",
        "\n",
        "        # Prepare input texts and available option labels\n",
        "        input_texts = []\n",
        "        available_option_labels = []\n",
        "        for label in option_labels:\n",
        "            if label in options and options[label]:\n",
        "                option_text = options[label]\n",
        "                input_text = f\"{prompt} {question} Option {label}: {option_text}\"\n",
        "                input_texts.append(input_text)\n",
        "                available_option_labels.append(label)\n",
        "\n",
        "        if not input_texts:\n",
        "            continue  # Skip if there are no valid options\n",
        "\n",
        "        # Tokenize all options for the current question\n",
        "        inputs = tokenizer(\n",
        "            input_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        # Run the model on the options\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "            # Compute log probabilities\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "            gathered_log_probs = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
        "            total_log_probs = gathered_log_probs.sum(dim=1).cpu().numpy()\n",
        "\n",
        "        # Get the indices sorted by score in descending order\n",
        "        sorted_indices = np.argsort(-total_log_probs)\n",
        "\n",
        "        # Select the top option(s)\n",
        "        top_k = k  # Change to 3 if you want top 3 options\n",
        "        top_indices = sorted_indices[:top_k]\n",
        "        predicted_options = [available_option_labels[i] for i in top_indices]\n",
        "\n",
        "        result = {\n",
        "            \"id\": entry[\"id\"],\n",
        "            \"question\": question,\n",
        "            \"predicted_options\": predicted_options  # List of top options\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "6T3Lv6L4ZVfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   100mins for CPU\n",
        "*   44s for A100 inference for 200 questions, 19mins for Cloze inference, 10mins for MCP inference\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lRUWRygzoet0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results1=make_inference_on_mcqa_mcp_generate(model, tokenizer, data, k=3)"
      ],
      "metadata": {
        "id": "pSajVXUipIwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results2=make_inference_on_mcqa_mcp_generate2(model, tokenizer, data, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2LI6Mxl2JhX",
        "outputId": "2deddc38-97a9-4771-c428-1a1b1af470a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results = make_inference_on_mcqa_cloze(model, tokenizer, data,3)"
      ],
      "metadata": {
        "id": "0z_IZgBZoGql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=calculate_accuracy(inference_results, data)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRRAe5ZAb3-0",
        "outputId": "32b4472d-4ad3-48c5-949e-d21dd0f98207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy2=calculate_accuracy(inference_results2, data)\n",
        "print(f\"Accuracy: {accuracy2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apm0raOQEvHY",
        "outputId": "9074a707-76dd-4787-fb0d-ef904fde33ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No prediction for question ID: Q28\n",
            "No prediction for question ID: Q190\n",
            "Accuracy: 0.535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAP33=calculate_map_at_k(inference_results2, data, k=3)\n",
        "print(f\"MAP@3: {MAP33}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkzUVk3gcTm7",
        "outputId": "55576fe2-f9fd-4629-cfb0-ee704d934269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@3: 0.6499999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=calculate_accuracy(inference_results1, data)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u2J8tDLs4NB",
        "outputId": "ac5255c8-5b51-4500-c6fe-da38629113ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAP32=calculate_map_at_k(inference_results1, data, k=3)\n",
        "print(f\"MAP@3: {MAP32}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yZhTa-ZcOJJ",
        "outputId": "23a7d917-3910-4388-8754-78fcbed66843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@3: 0.6741666666666662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=calculate_accuracy(inference_results, data)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBxkCcLk20-b",
        "outputId": "6b579664-6172-43ca-c4eb-1c20572facb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAP3=calculate_map_at_k(inference_results, data, k=3)\n",
        "print(f\"MAP@3: {MAP3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIIMzR_uWvfS",
        "outputId": "559289f7-34cd-41ce-9c04-81b682891d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@3: 0.4633333333333331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the inference results to a DataFrame\n",
        "df = pd.DataFrame(inference_results)\n",
        "\n",
        "# Count occurrences of each predicted option\n",
        "option_counts = df['predicted_option'].value_counts()\n",
        "\n",
        "# Plot the distribution of predicted options\n",
        "plt.figure(figsize=(8, 5))\n",
        "option_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Predicted Answers')\n",
        "plt.xlabel('Answer Option')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "j5WrPANkitkZ",
        "outputId": "b57735bd-55a2-419e-f0f3-0980ab0c3e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_dict = {}\n",
        "data_moved_dict = {}\n",
        "\n",
        "option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "Acc_list = []\n",
        "\n",
        "for target_option_label in option_labels:\n",
        "    # Transform data\n",
        "    transformed_data = transform_data(data, target_option_label)\n",
        "    data_moved_dict[target_option_label] = transformed_data\n",
        "\n",
        "    # Run inference\n",
        "    results = make_inference_on_mcqa_batch(model, tokenizer, transformed_data,1)\n",
        "    inference_results_dict[target_option_label] = results\n",
        "\n",
        "    # Compute accuracy\n",
        "    Acc_i = calculate_accuracy(results, transformed_data)\n",
        "    Acc_list.append(Acc_i)\n",
        "    print(f\"Accuracy when correct answers are moved to '{target_option_label}': {Acc_i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SijMieZi3DsP",
        "outputId": "05f1a9f0-58ef-4337-a083-84d983a8283f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when correct answers are moved to 'A': 0.33\n",
            "Accuracy when correct answers are moved to 'B': 0.26\n",
            "Accuracy when correct answers are moved to 'C': 0.24\n",
            "Accuracy when correct answers are moved to 'D': 0.33\n",
            "Accuracy when correct answers are moved to 'E': 0.415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_dict = {}\n",
        "data_moved_dict = {}\n",
        "\n",
        "option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "Acc_list = []\n",
        "\n",
        "for target_option_label in option_labels:\n",
        "    # Transform data\n",
        "    transformed_data = transform_data(data, target_option_label)\n",
        "    data_moved_dict[target_option_label] = transformed_data\n",
        "\n",
        "    # Run inference\n",
        "    results = make_inference_on_mcqa_mcp_generate(model, tokenizer, transformed_data,1)\n",
        "    inference_results_dict[target_option_label] = results\n",
        "\n",
        "    # Compute accuracy\n",
        "    Acc_i = calculate_accuracy(results, transformed_data)\n",
        "    Acc_list.append(Acc_i)\n",
        "    print(f\"Accuracy when correct answers are moved to '{target_option_label}': {Acc_i}\")"
      ],
      "metadata": {
        "id": "CaMsDH4rrJBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5680c6a9-d4f5-4b65-a27e-815472272b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when correct answers are moved to 'A': 0.52\n",
            "Accuracy when correct answers are moved to 'B': 0.665\n",
            "Accuracy when correct answers are moved to 'C': 0.52\n",
            "Accuracy when correct answers are moved to 'D': 0.84\n",
            "Accuracy when correct answers are moved to 'E': 0.035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initial μ_bias: 5.3\n",
        "\n",
        "MCQ bias:μ_bias: 0.198. 19.8"
      ],
      "metadata": {
        "id": "uf6I6npSWvBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate μ_bias\n",
        "Acc_0=0.56\n",
        "Acc_diffs = [abs(Acc_i - Acc_0) for Acc_i in Acc_list]\n",
        "mu_bias = sum(Acc_diffs) / len(option_labels)\n",
        "print(f\"μ_bias: {mu_bias}\")"
      ],
      "metadata": {
        "id": "SBy4ghwVdb52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Acc_0=0.56\n",
        "Acc_diffs = [abs(Acc_i - Acc_0) for Acc_i in Acc_list]\n",
        "mu_bias = sum(Acc_diffs) / len(option_labels)\n",
        "print(f\"μ_bias: {mu_bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQIXSjVo587b",
        "outputId": "9c0133b7-bee5-4126-8526-8d685257b894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "μ_bias: 0.198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GECru5CRB4o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {}\n",
        "for option, transformed_data in transformed_datasets.items():\n",
        "    predictions = make_inference_on_mcqa(transformed_data)\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    accuracy_merged_df = pd.merge(predictions_df, correct_answers_df, on='id')\n",
        "    accuracy = (accuracy_merged_df['predicted_option'] == accuracy_merged_df['Correct Answer']).mean()\n",
        "    accuracies[option] = accuracy\n",
        "    print(f\"Accuracy with correct answer in position {option}: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeRMQIebvlr-",
        "outputId": "0be9b310-e265-430c-c909-5257a4d4edaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with correct answer in position A: 0.195\n",
            "Accuracy with correct answer in position B: 0.225\n",
            "Accuracy with correct answer in position C: 0.2\n",
            "Accuracy with correct answer in position D: 0.225\n",
            "Accuracy with correct answer in position E: 0.255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Mean Bias (µbias): 9.5"
      ],
      "metadata": {
        "id": "3ykxF9YEfEzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_accuracy = (merged_df['predicted_option'] == merged_df['Correct Answer']).mean()\n",
        "mean_bias = sum(abs(acc - original_accuracy) for acc in accuracies.values()) / len(accuracies) *100\n",
        "print(f\"Mean Bias (µbias): {mean_bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz1a39XeGpJW",
        "outputId": "470da92e-c935-4875-8474-b0c21c4917ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Bias (µbias): 9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "YxzIFEMv_F0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "gfFbnFeSpMpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequences_with_symbol_and_content(row):\n",
        "    prompt = row['prompt']\n",
        "    choices = {\n",
        "        'A': row['A'],\n",
        "        'B': row['B'],\n",
        "        'C': row['C'],\n",
        "        'D': row['D'],\n",
        "        'E': row['E']\n",
        "    }\n",
        "    answer_symbol = row['answer']  # This will be 'A', 'B', 'C', 'D', or 'E'\n",
        "    answer_content = choices[answer_symbol]\n",
        "    correct_answer = f\"{answer_symbol}) {answer_content}\"\n",
        "\n",
        "    # Create the options text\n",
        "    options_text = '\\n'.join([f\"{symbol}) {content}\" for symbol, content in choices.items()])\n",
        "\n",
        "    # Create the input sequence\n",
        "    input_sequence = f\"Question: {prompt}\\nOptions:\\n{options_text}\\nAnswer:\"\n",
        "\n",
        "    # The target output is the symbol and content of the correct answer\n",
        "    target_output = correct_answer\n",
        "\n",
        "    return input_sequence, target_output\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples['input_sequence'],\n",
        "        max_length=2048,\n",
        "        truncation=True,\n",
        "        padding=True,  # Do not pad here\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples['target_output'],\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True,  # Do not pad here\n",
        "        )\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "fdkezVZycG2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "target_outputs = []\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    input_seq, target_output = prepare_sequences_with_symbol_and_content(row)\n",
        "    input_sequences.append(input_seq)\n",
        "    target_outputs.append(target_output)"
      ],
      "metadata": {
        "id": "KCrAa-1qjCi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    'input_sequence': input_sequences,\n",
        "    'target_output': target_outputs,\n",
        "}\n",
        "hf_dataset = Dataset.from_dict(data_dict)\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=['input_sequence', 'target_output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a4b043312e2341d7b2267f4d96414d1a",
            "f9fb44d28a6e4d47bb89db3058862aa2",
            "179679aca202453c9e4b4698ce23c129",
            "d6fba254425f4f46904bcf268cb86cb5",
            "8c68d0ea0fd941a0b0fa18921a83ef82",
            "b8d4cd8a0f1141b6820631754bc03e08",
            "1662cdacff254b0eb7b8761f7684cb3c",
            "4a860c57965649449498f35312f70adf",
            "d5f57485cda64ca5bcbe316187662f84",
            "eb9fdfbfac3c415d991994d05429ecd9",
            "f826b9d9aa6c4b04917aa5d077b501e1"
          ]
        },
        "id": "_ItFkemni6Ej",
        "outputId": "d4138844-fed9-469a-f2bb-ebd3ba3d9482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4b043312e2341d7b2267f4d96414d1a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding='longest'\n",
        ")"
      ],
      "metadata": {
        "id": "cnQtKh-ueh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(row):\n",
        "    prompt = (\n",
        "        \"Answer the following question by selecting one of the options. \"\n",
        "        \"Your response should be in the format {option}, where {option} is A, B, C, D, or E.\\n\\n\"\n",
        "        f\"Question: {row['prompt']}\\n\"\n",
        "        \"Options:\\n\"\n",
        "        f\"A) {row['A']}\\n\"\n",
        "        f\"B) {row['B']}\\n\"\n",
        "        f\"C) {row['C']}\\n\"\n",
        "        f\"D) {row['D']}\\n\"\n",
        "        f\"E) {row['E']}\\n\"  # Include option E if applicable\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    return prompt\n",
        "# Prepare the prompts and answers\n",
        "train['prompt'] = train.apply(create_prompt, axis=1)\n",
        "train['answer_text'] = train['answer'].str.strip().str.upper()\n",
        "# Create the dataset\n",
        "dataset = Dataset.from_pandas(train[['prompt', 'answer_text']])\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    prompts = examples['prompt']\n",
        "    answers = examples['answer_text']\n",
        "\n",
        "    # Tokenize the prompts\n",
        "    tokenized_prompts = tokenizer(\n",
        "        prompts,\n",
        "        padding='max_length',  # Ensure consistent length\n",
        "        truncation=False,\n",
        "        max_length=1024,  # Adjust based on your requirements\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Prepare answer contents with symbols\n",
        "    answer_contents = []\n",
        "    for i in range(len(answers)):\n",
        "        symbol = answers[i] + ')'  # Ensure symbols are added (e.g., 'D)')\n",
        "        content = train.loc[i, answers[i]]  # Extract the content corresponding to the symbol\n",
        "        answer_contents.append(f\"{symbol} {content}\")  # Combine symbol + content\n",
        "\n",
        "    # Tokenize the answers\n",
        "    tokenized_answers = tokenizer(\n",
        "        answer_contents,\n",
        "        padding='max_length',  # Ensure consistent length\n",
        "        truncation=False,\n",
        "        max_length=512,  # Adjust based on symbol + content length\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Create labels\n",
        "    labels = []\n",
        "    for i in range(len(prompts)):\n",
        "        # Number of tokens in the prompt (non-padded)\n",
        "        prompt_length = sum(tokenized_prompts['attention_mask'][i])\n",
        "\n",
        "        # Create a list of -100 for the prompt tokens\n",
        "        label = [-100] * prompt_length\n",
        "\n",
        "        # Get the answer token IDs, removing padding\n",
        "        answer_ids = [\n",
        "            token_id for token_id in tokenized_answers['input_ids'][i]\n",
        "            if token_id != tokenizer.pad_token_id\n",
        "        ]\n",
        "\n",
        "        # Append answer tokens to labels\n",
        "        label += answer_ids\n",
        "\n",
        "        # Truncate or pad labels to match max_length\n",
        "        label = label[:512] + [-100] * (512 - len(label))\n",
        "        labels.append(label)\n",
        "\n",
        "    return {\n",
        "        'input_ids': tokenized_prompts['input_ids'],\n",
        "        'attention_mask': tokenized_prompts['attention_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ac5c7fce22a349daaa1d7f76c7aa002b",
            "533a0c97ded643c99d782b63d8057462",
            "4c0d564c102a4977af3615dbfb2697e9",
            "a0f192b7cc2349c787507f8f2b5916c8",
            "a032035f90924119ae916fb070cbb168",
            "ff0456959fda46eea106cea499865cc2",
            "8618bc4f8e4a44f3a4ebf5a64ce831e3",
            "526172eaa2ba4644999608fc098fdb0c",
            "3645e72959b948e89d7262e3eb818d60",
            "912e45770df645b68edd1562bd29d2d8",
            "7cce2b63300d40129857abdf3ee0e19b"
          ]
        },
        "id": "jeKrYBSKaDla",
        "outputId": "528bb20e-957a-4c21-df38-aa8f580e1a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5c7fce22a349daaa1d7f76c7aa002b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decode input_ids and labels\n",
        "def decode_example(example, tokenizer):\n",
        "    input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
        "    label_ids = example['labels']\n",
        "\n",
        "    # Replace -100 with pad_token_id for decoding labels\n",
        "    label_ids_decodable = [token_id if token_id != -100 else tokenizer.pad_token_id for token_id in label_ids]\n",
        "    label_text = tokenizer.decode(label_ids_decodable, skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        'input_text': input_text,\n",
        "        'label_text': label_text\n",
        "    }\n",
        "\n",
        "# Decode the first two examples\n",
        "decoded_examples = [decode_example(tokenized_dataset[i], tokenizer) for i in range(2)]\n",
        "\n",
        "for idx, example in enumerate(decoded_examples):\n",
        "    print(f\"Example {idx+1}:\")\n",
        "    print(\"Input Text:\")\n",
        "    print(example['input_text'])\n",
        "    print(\"\\nLabel Text:\")\n",
        "    print(example['label_text'])\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pngbzOQOSbl7",
        "outputId": "87fa53a9-1a71-4ce4-b3cc-4e32040a0f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Input Text:\n",
            "Question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
            "Options:\n",
            "A) MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
            "B) MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
            "C) MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
            "D) MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
            "E) MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
            "Answer:\n",
            "\n",
            "Label Text:\n",
            "D) MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
            "--------------------------------------------------\n",
            "Example 2:\n",
            "Input Text:\n",
            "Question: Which of the following is an accurate definition of dynamic scaling in self-similar systems?\n",
            "Options:\n",
            "A) Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.\n",
            "B) Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is similar to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.\n",
            "C) Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.\n",
            "D) Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.\n",
            "E) Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is independent of the respective data taken from snapshots of any earlier or later time. This independence is tested by a certain time-dependent stochastic variable z.\n",
            "Answer:\n",
            "\n",
            "Label Text:\n",
            "A) Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "tokenized_train_dataset = split_tokenized_dataset['train']\n",
        "tokenized_valid_dataset = split_tokenized_dataset['test']"
      ],
      "metadata": {
        "id": "eYbyTzX81z1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUjhhxMAAyDS",
        "outputId": "521c29c2-1303-46e8-b64b-027964a8d917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove labels and test forward pass\n",
        "inputs_no_labels = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
        "outputs = model.forward(**inputs_no_labels)\n",
        "print(\"Logits from forward without labels:\", outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PMf4KH3EG3_",
        "outputId": "f42ee682-e61e-4dbc-e23f-70bd17f49de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits from forward without labels: tensor([[[ 3.4375,  3.1562,  3.4375,  ..., -4.9375, -4.9375, -4.9375],\n",
            "         [ 6.1875,  4.6250,  4.5938,  ..., -4.0938, -4.0938, -4.0938],\n",
            "         [ 8.4375,  8.1875,  7.0625,  ..., -4.7812, -4.7812, -4.7812],\n",
            "         [12.0625, 10.0625, 12.3750,  ..., -4.6250, -4.6250, -4.6250]]],\n",
            "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs, return_dict=True)\n",
        "print(\"Loss:\", outputs.loss)\n",
        "print(\"Logits:\", outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDBCgadVFaV9",
        "outputId": "937900f5-51fe-4cf2-bad6-3cdce9065c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(9.1318, device='cuda:0', grad_fn=<LinearCrossEntropyFunctionBackward>)\n",
            "Logits: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.forward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_IHOC82F3sA",
        "outputId": "4fdec7db-d7b0-4190-c759-216db8e0c58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method convert_outputs_to_fp32.<locals>.forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test how the tokenizer handles the symbols\n",
        "symbols = ['A', 'B', 'C', 'D', 'E']\n",
        "for symbol in symbols:\n",
        "    token_ids = tokenizer.encode(symbol, add_special_tokens=False)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    print(f\"Symbol: {symbol}, Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxWiVd-AlQN9",
        "outputId": "ce595ed8-b4fd-4e6b-ec07-f1fd13fadaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbol: A, Tokens: ['A']\n",
            "Symbol: B, Tokens: ['B']\n",
            "Symbol: C, Tokens: ['C']\n",
            "Symbol: D, Tokens: ['D']\n",
            "Symbol: E, Tokens: ['E']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ['A:', 'A)']\n",
        "for seq in sequences:\n",
        "    token_ids = tokenizer.encode(seq, add_special_tokens=False)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    print(f\"Sequence: {seq}, Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2pvLNMcmVvf",
        "outputId": "95858f55-0a0d-4a6c-959f-492088852193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: A:, Tokens: ['A', ':']\n",
            "Sequence: A), Tokens: ['A', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RSCBDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, max_length=2048):\n",
        "        if isinstance(samples, pd.DataFrame):\n",
        "            self.samples = samples.to_dict(orient='records')\n",
        "        else:\n",
        "            self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if isinstance(idx, int):\n",
        "          return self.process_sample(self.samples[idx])\n",
        "      elif isinstance(idx, list):\n",
        "          batch = {}\n",
        "          # Initialize lists for each key in the sample\n",
        "          keys = ['input_ids', 'attention_mask', 'labels', 'symbol_mask', 'symbol_token_id']\n",
        "          for key in keys:\n",
        "              batch[key] = []\n",
        "          # Process each sample and collect the results\n",
        "          for i in idx:\n",
        "              sample = self.process_sample(self.samples[i])\n",
        "              for key in keys:\n",
        "                  batch[key].append(sample[key])\n",
        "          return batch\n",
        "      else:\n",
        "          raise TypeError(f\"Unsupported index type: {type(idx)}\")\n",
        "\n",
        "    def process_sample(self, sample):\n",
        "        # Construct the input_text\n",
        "        input_text = sample['prompt'] + \"\\n\"\n",
        "        for option_label in ['A', 'B', 'C', 'D', 'E']:\n",
        "            option_text = sample.get(option_label, \"\")\n",
        "            if option_text:\n",
        "                input_text += f\"{option_label}: {option_text}\\n\"\n",
        "\n",
        "        input_text += \"Answer:\\n\"\n",
        "\n",
        "        # Construct the output_text\n",
        "        correct_option_label = sample['answer']\n",
        "        correct_option_text = sample.get(correct_option_label, \"\")\n",
        "        output_text = f\"{correct_option_label}: {correct_option_text}\"\n",
        "\n",
        "        # Concatenate input and output for causal language modeling\n",
        "        full_text = input_text + output_text\n",
        "\n",
        "        # Tokenize full text\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "        # Mask input tokens\n",
        "        input_prefix_length = len(self.tokenizer.encode(input_text, add_special_tokens=False))\n",
        "        labels[:input_prefix_length] = -100  # Mask input tokens\n",
        "\n",
        "        # Identify the symbol token in the output\n",
        "        output_ids = input_ids[input_prefix_length:]\n",
        "        # The first token in the output should be the symbol (e.g., 'D')\n",
        "        symbol_token_id = output_ids[0]\n",
        "        symbol_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "        symbol_position = input_prefix_length  # Position of the symbol token\n",
        "        symbol_mask[symbol_position] = True\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "            'symbol_mask': symbol_mask,\n",
        "            'symbol_token_id': symbol_token_id  # For computing p_s\n",
        "        }"
      ],
      "metadata": {
        "id": "9NxomvE6rVrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(features):\n",
        "    batch = {}\n",
        "    keys = features[0].keys()\n",
        "    for key in keys:\n",
        "        sequences = [feature[key] for feature in features]\n",
        "        if key == 'input_ids':\n",
        "            padding_value = tokenizer.pad_token_id\n",
        "            batch[key] = torch.nn.utils.rnn.pad_sequence(\n",
        "                sequences, batch_first=True, padding_value=padding_value\n",
        "            )\n",
        "        elif key == 'labels':\n",
        "            padding_value = -100\n",
        "            batch[key] = torch.nn.utils.rnn.pad_sequence(\n",
        "                sequences, batch_first=True, padding_value=padding_value\n",
        "            )\n",
        "        elif key in ['attention_mask', 'symbol_mask']:\n",
        "            padding_value = 0\n",
        "            batch[key] = torch.nn.utils.rnn.pad_sequence(\n",
        "                sequences, batch_first=True, padding_value=padding_value\n",
        "            )\n",
        "        elif key == 'symbol_token_id':\n",
        "            batch[key] = torch.stack(sequences)\n",
        "    return batch"
      ],
      "metadata": {
        "id": "9xkduSlrn3wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSCBTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop('labels')\n",
        "        symbol_mask = inputs.pop('symbol_mask').to(model.device)\n",
        "        symbol_token_id = inputs.pop('symbol_token_id').to(model.device)\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "        # Shift logits and labels for causal language modeling\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        symbol_mask = symbol_mask[..., 1:].contiguous()\n",
        "\n",
        "        # Flatten the tokens\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=-100)\n",
        "        loss = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1)\n",
        "        )\n",
        "        loss = loss.view(shift_labels.size())\n",
        "\n",
        "        # Compute L_SCB (standard CrossEntropyLoss over all output tokens)\n",
        "        l_scb = loss.mean()\n",
        "\n",
        "        # Compute p_s (predicted probability of the symbol token)\n",
        "        # Identify positions where symbol_mask is True\n",
        "        symbol_positions = symbol_mask.nonzero(as_tuple=True)\n",
        "        symbol_logits = shift_logits[symbol_positions]  # Shape: (num_symbols, vocab_size)\n",
        "        symbol_probs = torch.softmax(symbol_logits, dim=-1)  # Get probabilities\n",
        "        p_s = symbol_probs.gather(1, symbol_token_id.unsqueeze(-1)).squeeze(-1)  # Shape: (num_symbols,)\n",
        "\n",
        "        # Compute the additional loss term\n",
        "        alpha = 2.0  # Focusing parameter (adjust as needed)\n",
        "        beta = 0.1   # Re-assigned weight for the symbol token (adjust as needed)\n",
        "\n",
        "        # Compute the Focal Loss component\n",
        "        focal_weight = (1 - p_s) ** alpha\n",
        "        symbol_loss = focal_weight * beta * (-torch.log(p_s + 1e-8))  # Add epsilon to prevent log(0)\n",
        "\n",
        "        # Total loss\n",
        "        loss = l_scb + symbol_loss.mean()\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, outputs\n",
        "        else:\n",
        "            return loss"
      ],
      "metadata": {
        "id": "hlezthNuoH1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = RSCBDataset(train, tokenizer, max_length=2048)"
      ],
      "metadata": {
        "id": "Y9Ot_lkToU-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_num_workers=0,# Use this for WandB etc\n",
        "    )\n",
        "# Initialize trainer\n",
        "trainer = RSCBTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGHHtaDfojgl",
        "outputId": "23ead36d-e14a-4600-e8f6-a22b76a87aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ],
      "metadata": {
        "id": "mv_T1koVtHOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0gUMuagDols7",
        "outputId": "63fa18d9-f001-4826-d579-7a01dda1dff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 100\n",
            " \"-____-\"     Number of trainable parameters = 2,818,747,392\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:41, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.477200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.302700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.073400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.036200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.016100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.031600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.010300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.020500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.024700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.006700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.008300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.004400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.007100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.043700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.005700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.004400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.005800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.014600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.1382784746706966, metrics={'train_runtime': 104.2246, 'train_samples_per_second': 7.676, 'train_steps_per_second': 0.959, 'total_flos': 3966823204761600.0, 'train_loss': 0.1382784746706966, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_tokens = ['A', 'B', 'C', 'D', 'E']\n",
        "special_tokens = {'additional_special_tokens': symbol_tokens}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n",
        "print(f\"Added {num_added_tokens} new tokens.\")\n",
        "symbol_token_ids = tokenizer.convert_tokens_to_ids(symbol_tokens)\n",
        "print(\"Symbol Token IDs:\", symbol_token_ids)\n",
        "\n",
        "# 4. Verify Single-Token Symbols\n",
        "for symbol in symbol_tokens:\n",
        "    token_ids = tokenizer.encode(symbol, add_special_tokens=False)\n",
        "    print(f\"Symbol: {symbol}, Token IDs: {token_ids}\")\n",
        "    assert len(token_ids) == 1, f\"Symbol '{symbol}' is tokenized into multiple tokens: {token_ids}\"\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "def create_symbol_mask(labels, symbol_token_ids):\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    symbol_token_ids_tensor = torch.tensor(symbol_token_ids).to(labels_tensor.device)\n",
        "    symbol_mask = torch.isin(labels_tensor, symbol_token_ids_tensor)\n",
        "    return symbol_mask\n",
        "\n",
        "def compute_rscb_loss(logits, labels, symbol_mask, alpha=2.0, beta=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Reweighting Symbol-Content Binding (RSCB) loss.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): Logits from the model of shape (batch_size, seq_length, vocab_size).\n",
        "        labels (torch.Tensor): Ground truth labels of shape (batch_size, seq_length).\n",
        "        symbol_mask (torch.Tensor): Binary mask indicating symbol tokens of shape (batch_size, seq_length).\n",
        "        alpha (float): Exponent for reweighting.\n",
        "        beta (float): Weighting factor for the symbol tokens.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The computed RSCB loss.\n",
        "    \"\"\"\n",
        "    # Flatten the tensors\n",
        "    logits_flat = logits.view(-1, logits.size(-1))  # (batch_size * seq_length, vocab_size)\n",
        "    labels_flat = labels.view(-1)  # (batch_size * seq_length)\n",
        "    symbol_mask_flat = symbol_mask.view(-1)  # (batch_size * seq_length)\n",
        "\n",
        "    # Compute standard cross-entropy loss without reduction\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
        "    ce_loss = loss_fn(logits_flat, labels_flat)  # (batch_size * seq_length)\n",
        "\n",
        "    # Compute probabilities\n",
        "    probs = torch.softmax(logits_flat, dim=-1)  # (batch_size * seq_length, vocab_size)\n",
        "    target_probs = probs[torch.arange(probs.size(0)), labels_flat]  # (batch_size * seq_length)\n",
        "\n",
        "    # Identify valid indices (labels not ignored)\n",
        "    valid_indices = labels_flat != -100\n",
        "    target_probs = target_probs[valid_indices]  # (num_valid)\n",
        "    symbol_mask_flat = symbol_mask_flat[valid_indices]  # (num_valid)\n",
        "\n",
        "    # Extract probabilities for symbol tokens\n",
        "    ps = target_probs[symbol_mask_flat.bool()]  # (num_symbol_tokens)\n",
        "\n",
        "    # Compute the RSCB additional loss term\n",
        "    rscb_term = (1 - ps) ** alpha * beta * (-torch.log(ps))  # (num_symbol_tokens)\n",
        "\n",
        "    # Aggregate the losses\n",
        "    # Start with the standard cross-entropy loss for valid indices\n",
        "    total_loss = ce_loss[valid_indices].clone()  # (num_valid)\n",
        "\n",
        "    # Add the RSCB term only to symbol tokens\n",
        "    total_loss[symbol_mask_flat.bool()] += rscb_term  # (num_valid)\n",
        "\n",
        "    # Compute the mean loss\n",
        "    mean_loss = total_loss.mean()\n",
        "    return mean_loss"
      ],
      "metadata": {
        "id": "VhfRSQgdyWeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9545535-3ac3-471c-ecf2-7f7260aaf22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 5 new tokens.\n",
            "Symbol Token IDs: [128256, 128257, 128258, 128259, 128260]\n",
            "Symbol: A), Token IDs: [128256]\n",
            "Symbol: B), Token IDs: [128257]\n",
            "Symbol: C), Token IDs: [128258]\n",
            "Symbol: D), Token IDs: [128259]\n",
            "Symbol: E), Token IDs: [128260]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, symbol_token_ids, alpha=2.0, beta=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.symbol_token_ids = symbol_token_ids\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "\n",
        "      labels = inputs.get(\"labels\")\n",
        "      if labels is None:\n",
        "          raise ValueError(\"Labels are missing from the inputs.\")\n",
        "      inputs_without_labels = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
        "      outputs = model(**inputs_without_labels)\n",
        "    # Forward pass\n",
        "      #outputs = model(**inputs)\n",
        "      #print(\"Outputs:\", outputs)  # Debug the outputs\n",
        "\n",
        "      logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.get(\"logits\", None)\n",
        "      if logits is None:\n",
        "          raise ValueError(\"Logits are missing from the model outputs.\")\n",
        "\n",
        "    # Create symbol mask\n",
        "      symbol_mask = create_symbol_mask(labels, self.symbol_token_ids).to(labels.device)\n",
        "\n",
        "    # Compute custom RSCB loss\n",
        "      loss = compute_rscb_loss(\n",
        "          logits,\n",
        "          labels,\n",
        "          symbol_mask,\n",
        "          alpha=self.alpha,\n",
        "          beta=self.beta,\n",
        "      )\n",
        "\n",
        "      return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "BgPnBJ3YFoO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEbnd2PsrZF9",
        "outputId": "4eadac81-9798-496e-b5fa-0f661f3059eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        remove_unused_columns=False,# Use this for WandB etc\n",
        "    )\n",
        "\n",
        "# Initialize the CustomTrainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    symbol_token_ids=symbol_token_ids,\n",
        "    alpha=2.0,                        # Adjust based on hyperparameter tuning\n",
        "    beta=0.1,                         # Adjust based on hyperparameter tuning\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,  # Your tokenized dataset\n",
        "    data_collator=data_collator# Define a data collator if needed\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItQWgZgFJB-d",
        "outputId": "c7830ae5-eb9e-49b8-e394-284a904e59cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-98ece5e91428>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample labels (Assuming 'D' is the correct answer)\n",
        "sample_labels = tokenizer.encode(\"D) MOND is a theory that increases the discrepancy between the\", add_special_tokens=False)\n",
        "mask = create_symbol_mask(sample_labels, symbol_token_ids)\n",
        "print(f\"Sample Labels: {sample_labels}\")\n",
        "print(f\"Symbol Mask: {mask}\")\n",
        "print(f\"Number of Symbols in Labels: {mask.sum().item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIO__dXYHPVp",
        "outputId": "1956044c-6725-44fd-ea2b-fab02d9fa912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Labels: [128259, 29637, 35, 374, 264, 10334, 430, 12992, 279, 79105, 1990, 279]\n",
            "Symbol Mask: tensor([ True, False, False, False, False, False, False, False, False, False,\n",
            "        False, False])\n",
            "Number of Symbols in Labels: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure model is on the correct device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Move inputs to the same device as the model\n",
        "inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
        "\n",
        "# Pass inputs to the model\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "MOoAbfDOoCtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c7o-khrSJXPQ",
        "outputId": "4ebaa580-0005-4d43-ae67-0d70bf19a2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 100\n",
            " \"-____-\"     Number of trainable parameters = 2,818,747,392\n",
            "<ipython-input-17-4b813ab6fae8>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_tensor = torch.tensor(labels)\n",
            "<ipython-input-17-4b813ab6fae8>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_tensor = torch.tensor(labels)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 02:04, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>41.093500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>40.628600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>39.172300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>41.179400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>40.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>40.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>40.259300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>39.754300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>44.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>40.210100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>40.222100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>40.782800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>39.358000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>42.115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>40.462900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>41.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>39.701000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>41.099800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>40.269400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>39.811500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>40.943400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>41.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>40.730100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>44.528800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>39.638400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>41.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>40.514900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>39.248600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>40.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>41.746400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>41.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>39.246000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>41.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>43.649700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>41.756900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>40.445200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>39.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>40.108500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>40.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>42.135500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>40.848200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>39.855200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>41.114300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>38.901700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>42.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>40.054400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>42.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>39.419600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>38.758500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>40.464400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>40.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>42.773200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>41.064800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>40.105000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>39.239100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>40.105200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>41.161600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>39.245700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>39.944600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>40.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>40.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>40.569500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>41.169700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>39.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>41.968200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>39.713200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>40.838400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>41.474100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>39.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>41.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>40.851200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>41.157200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>40.417600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>41.024000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>43.812300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>42.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>39.746700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>39.907900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>40.274700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>40.527300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>46.604600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>40.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>40.287700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>40.415900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>40.167200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>39.658800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>40.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>41.926500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>39.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>39.336600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>39.747500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>42.655000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>41.231200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>42.632700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>40.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>39.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>40.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>40.763400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>40.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>38.389900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=40.75544761657715, metrics={'train_runtime': 129.5987, 'train_samples_per_second': 6.173, 'train_steps_per_second': 0.772, 'total_flos': 8415652213555200.0, 'train_loss': 40.75544761657715, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_distribution = train['answer'].value_counts()\n",
        "\n",
        "# Print the distribution\n",
        "print(answer_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvMutKbpgMuG",
        "outputId": "dcccad1e-fbd7-4a25-ff1f-0a876064dc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer\n",
            "B    48\n",
            "C    44\n",
            "D    38\n",
            "A    37\n",
            "E    33\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Disable masked language modeling\n",
        ")"
      ],
      "metadata": {
        "id": "RCrniE0dbNlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding='longest',\n",
        "    label_pad_token_id=-100,\n",
        "    return_tensors='pt',\n",
        ")"
      ],
      "metadata": {
        "id": "AF3_HXtCBypN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "f5fa307608a04af28e9a14ba856f6a46",
            "b6352da4ea984e139ff7bef8cdb22951",
            "57984a4c7f7a4dd695e44438a4a03ac3",
            "cf2714008f224369ad5ca48403a57516",
            "7c98c8e174724e138ea1aed5baf281eb",
            "5d0a27730b9c46058ca0997ad2bfcdef",
            "c86e8f3b58774ff5ab60a07d2a901472",
            "e3e1e5a962d348b0aafc34b05c7747b4",
            "31727d0f86124b4e84c1bc6152f909f3",
            "e6c82257ece44b128c229132cd8517ee",
            "98e1441d414840908635d4201e25e7ce",
            "aacf7b2da1be443686d214817f6dc08d",
            "7e394bcf28e94e5da38cae97e02ed5fe",
            "a33e43e656684fa38374a75a914a2ad1",
            "d98fe590dcfb4e00a3222efd121e5369",
            "52fb1bf836174b20891bda9b010f5a80",
            "ec62e55d078d4b3d9d430ec1d80d4fb4",
            "7e6d39f455294ecdb5561536ed110d27",
            "34e3da16f21c4a7cb3ba2ba40be98668",
            "913ab0ba8bb8457bb5c4443ab88208bc",
            "728ddcdb109e46d9adc34cc1a406f5e6",
            "73cb3b908d57427282bca430fedb6cd5",
            "1dcd7b7ee1ff42259b44076c7992b9f4",
            "1d3f7fb1679542b794e5609ce99899f4",
            "14046e694f9543a3b626a36c581af24c",
            "d8f8de58884a44c29ea10a985855524d",
            "2c9e048242c74c9a9391ff01e6f90f14",
            "3a850c782f82498bbbd4c7d6c7c5184c",
            "a2a143e20b324202a440c88a52e374c9",
            "c1891556a846413bab4f29913275395e",
            "16422f7716354f82b6c992281e3900a5",
            "1a974e065f9a4661a2702644f97765a9",
            "b81d7552b17d405aaf03781cb3ecfb04",
            "0781c9e221774965994b51d1c5bb862c",
            "2206f186776e4c6e9c97c17e51a8ded2",
            "dd8a335a713346c884caa132b22656b4",
            "f15fc00779f243a1bc8b5af794f40c44",
            "aef0e8d71514467ebcab0236bf036c6b",
            "58b04428032e4b4497c39d820e9a7df4",
            "d3b5756f2c5c4181a6c45124344818e0",
            "ed40a4a39b2742ef91ee6c932c83d413",
            "50f032e46ef742a0adcc04570096145e",
            "10799e397d46458f83da4f60a098066d",
            "7863ea2471b54c2a9574a8d1a86ae226",
            "cfe51ccb7691422eb68b404fd2ddb1f1",
            "e9fbdb276f204aa0b200ea7997ea89b4",
            "812ee87a2a5b4c4baa69a44ff213975e",
            "260122f7c4cb48ca81593ab1b777c729",
            "c76c3b0f617e4aa88855765f85c38849",
            "82f0be2f16cc4b1c8ed1ad9cca3c49d7",
            "7e1cf2ed6f7c4e8b97b6089cf511ed82",
            "4198c8a1bbd44eaeae2e14f0d93e95c4",
            "6aa6dade958c4c0682232f44550d45cb",
            "e04ebf54b85a4da08cf093e06476da74",
            "09b6d50727374f7da77d76619138b3fe"
          ]
        },
        "id": "Emj5GyoWXsv9",
        "outputId": "9a426d17-797e-4dc1-ff33-940662d039e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5fa307608a04af28e9a14ba856f6a46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aacf7b2da1be443686d214817f6dc08d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dcd7b7ee1ff42259b44076c7992b9f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0781c9e221774965994b51d1c5bb862c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfe51ccb7691422eb68b404fd2ddb1f1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVtKhFPTYicH",
        "outputId": "aedfb0f5-c6d2-4202-a78b-ea7c40aad875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.11.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_template_with_context = \"\\n### Assistant:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
        "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
        "\n",
        "data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "l5w__BUL0HYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = tokenized_dataset,\n",
        "    max_seq_length = 2048,\n",
        "    data_collator = data_collator,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i41Xigkz2Yr7",
        "outputId": "41ee2e00-6fa5-4400-8ffc-aa1c50c841c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G9M9PL9jZW3H",
        "outputId": "2d0b3d9b-268a-4482-f4a6-72025df6b423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:30, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.065700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.329000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.264800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.040800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.389900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.096700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.883900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.842200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.807600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.219200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.861700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.847100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.029500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.660500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.861200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.860500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.809600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.637000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.877400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.819500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.725300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.786300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.995100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.811700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.653500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.790400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.821400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.803100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.550800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.574000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.607200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.863100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.771800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.594700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.819400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.601800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.700800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.709800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.058000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.521200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.649800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.959100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.832900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.691600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.787900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.588900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.537400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "yqL3uvNagYlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8,                        # Rank of the update matrices\n",
        "    lora_alpha=32,              # Scaling factor\n",
        "    target_modules=[\n",
        "    \"self_attn.q_proj\",\n",
        "    \"self_attn.k_proj\",\n",
        "],  # Modules to apply LoRA to\n",
        "    lora_dropout=0.05,          # Dropout rate\n",
        "    bias=\"none\",                # How to handle bias parameters\n",
        "    task_type=\"CAUSAL_LM\"       # The type of task\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "Zdl6PfGegRtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = '/content/drive/MyDrive/models/RSCB_LORA_llama3.5'\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDamXXcohijq",
        "outputId": "56b08de0-69b3-4d3a-b7ef-41b009813798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/models/RSCB_LORA_llama3.5/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/models/RSCB_LORA_llama3.5/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/models/RSCB_LORA_llama3.5/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig"
      ],
      "metadata": {
        "id": "7tuG4OmtjYWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save_directory = '/content/drive/MyDrive/models/RSCB_LORA_llama3.2'\n",
        "#lora_model = PeftModel.from_pretrained(model, save_directory)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "#tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "xU3H-s8ZjHQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save_directory = '/content/drive/MyDrive/models/RSCB_LORA_llama3.2'\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
        "\n",
        "# Load the tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "MQuCMgrHjEG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/models/RSCB_LORA_llama3.5\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "e56dadc3e86b49c79fc16ac7d215be70",
            "08ca1ef707e24e3eac4fc98597fd7e7e",
            "be584c95da6d45c1820774e28b534c74",
            "d1558a734a68436f89d2153141d87620",
            "f3e5305828044bae896f93044a6e12d1",
            "4d23252ff2d94378a2bd4d127261ec16",
            "dc87470c182a4376836100c228453bea",
            "6a391a9d1bd04d15909d7d722d22bb36",
            "3b22cdf0dc40420da873b7afe817c150",
            "4a05dd452e1b45af9070cf657e19a363",
            "7f66787c0b054927aa1918a9246b0358"
          ]
        },
        "id": "KRbFk6utk6NV",
        "outputId": "0ac956e8-629b-4fc3-a3cb-a2f9937f58e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e56dadc3e86b49c79fc16ac7d215be70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_trained = make_inference_on_mcqa_mcp_generate(model,tokenizer,data,3)"
      ],
      "metadata": {
        "id": "pEBg-YRg2Wq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_trained"
      ],
      "metadata": {
        "id": "DsxsnmD5lemW",
        "outputId": "ce0c759e-d4c3-4dc3-f03f-b95752d3f630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'Q0',\n",
              "  'question': 'Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q1',\n",
              "  'question': 'Which of the following is an accurate definition of dynamic scaling in self-similar systems?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q2',\n",
              "  'question': 'Which of the following statements accurately describes the origin and significance of the triskeles symbol?',\n",
              "  'predicted_options': ['A', 'C', 'E']},\n",
              " {'id': 'Q3',\n",
              "  'question': 'What is the significance of regularization in terms of renormalization problems in physics?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q4',\n",
              "  'question': 'Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q5',\n",
              "  'question': \"Which of the following statements accurately depicts the relationship between Gauss's law, electric flux, electric field, and symmetry in electric fields?\",\n",
              "  'predicted_options': ['B', 'A', 'C']},\n",
              " {'id': 'Q6',\n",
              "  'question': 'Which of the following statements accurately describes the dimension of an object in a CW complex?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q7',\n",
              "  'question': 'Which of the following statements accurately describes the blocking temperature of an antiferromagnetic layer in a spin valve?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q8',\n",
              "  'question': 'What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q9',\n",
              "  'question': 'What is the role of axioms in a formal theory?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q10',\n",
              "  'question': 'What did Fresnel predict and verify with regards to total internal reflections?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q11',\n",
              "  'question': 'What is the relationship between the Wigner function and the density matrix operator?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q12',\n",
              "  'question': 'What is one of the examples of the models proposed by cosmologists and theoretical physicists without the cosmological or Copernican principles that can be used to address specific issues in the Lambda-CDM model and distinguish between current models and other possible models?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q13',\n",
              "  'question': 'What is the Roche limit?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q14',\n",
              "  'question': \"What is Martin Heidegger's view on the relationship between time and human existence?\",\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q15',\n",
              "  'question': 'What is the \"ultraviolet catastrophe\"?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q16',\n",
              "  'question': 'What is the most popular explanation for the shower-curtain effect?',\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q17',\n",
              "  'question': 'What is the butterfly effect?',\n",
              "  'predicted_options': ['E', 'D', 'C']},\n",
              " {'id': 'Q18',\n",
              "  'question': \"What is the 'reactive Leidenfrost effect' observed in non-volatile materials?\",\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q19',\n",
              "  'question': 'What is reciprocal length or inverse length?',\n",
              "  'predicted_options': ['A', 'D', 'E']},\n",
              " {'id': 'Q20',\n",
              "  'question': 'Which of the following statements is true about the categorization of planetary systems according to their orbital dynamics?',\n",
              "  'predicted_options': ['D', 'E', 'C']},\n",
              " {'id': 'Q21',\n",
              "  'question': 'What is the propagation constant in sinusoidal waves?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q22',\n",
              "  'question': 'What is the gravitomagnetic interaction?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q23',\n",
              "  'question': \"What did Newton's manuscripts of the 1660s show?\",\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q24',\n",
              "  'question': 'What is the decay energy for the free neutron decay process?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q25',\n",
              "  'question': \"What is Hesse's principle of transfer in geometry?\",\n",
              "  'predicted_options': ['E', 'C', 'B']},\n",
              " {'id': 'Q26',\n",
              "  'question': 'What is the relationship between the Cauchy momentum equation and the Navier-Stokes equation?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q27',\n",
              "  'question': 'What is X-ray pulsar-based navigation (XNAV)?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q28',\n",
              "  'question': 'What is the evidence for the existence of a supermassive black hole at the center of the Milky Way galaxy?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q29',\n",
              "  'question': 'What is the function of the fibrous cardiac skeleton?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q30',\n",
              "  'question': 'What is the Carnot engine?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q31',\n",
              "  'question': 'Which mathematical function is commonly used to characterize linear time-invariant systems?',\n",
              "  'predicted_options': ['E', 'D', 'A']},\n",
              " {'id': 'Q32',\n",
              "  'question': 'What is the second law of thermodynamics?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q33',\n",
              "  'question': 'What are amorphous ferromagnetic metallic alloys, and what are their advantages?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q34',\n",
              "  'question': 'What is the Penrose process?',\n",
              "  'predicted_options': ['A', 'C', 'E']},\n",
              " {'id': 'Q35',\n",
              "  'question': 'What was the aim of the Gravity Probe B (GP-B) mission?',\n",
              "  'predicted_options': ['B', 'E', 'D']},\n",
              " {'id': 'Q36',\n",
              "  'question': \"What was Pierre de Fermat's solution to the problem of refraction?\",\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q37',\n",
              "  'question': 'What is the reason behind the adoption of a logarithmic scale of 5√100 ≈ 2.512 between magnitudes in astronomy?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q38',\n",
              "  'question': 'What is the spin quantum number?',\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q39',\n",
              "  'question': 'What is the synapstor or synapse transistor?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q40',\n",
              "  'question': 'What is spontaneous symmetry breaking?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q41',\n",
              "  'question': 'What is the proper distance for a redshift of 8.2?',\n",
              "  'predicted_options': ['C', 'A', 'B']},\n",
              " {'id': 'Q42',\n",
              "  'question': 'Who was the first to determine the velocity of a star moving away from the Earth using the Doppler effect?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q43',\n",
              "  'question': 'What is the information loss paradox in black holes?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q44',\n",
              "  'question': 'What is the Kutta condition?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q45',\n",
              "  'question': 'What is classical mechanics?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q46',\n",
              "  'question': 'Who shared the other half of the Nobel Prize with Yoichiro Nambu for discovering the origin of the explicit breaking of CP symmetry in the weak interactions?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q47',\n",
              "  'question': 'What are some models that attempt to account for all observations without invoking supplemental non-baryonic matter?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q48',\n",
              "  'question': 'What is the purpose of the proximity-focusing design in a RICH detector?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q49',\n",
              "  'question': 'What is a light-year?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q50',\n",
              "  'question': 'What is the main advantage of ferroelectric memristors?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q51',\n",
              "  'question': 'What is the term used to describe the conduction that occurs in non-crystalline semiconductors by charges quantum tunnelling from one localised site to another?',\n",
              "  'predicted_options': ['E', 'C', 'A']},\n",
              " {'id': 'Q52',\n",
              "  'question': 'What is resistivity?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q53',\n",
              "  'question': 'What did Newton adopt after his correspondence with Hooke in 1679-1680?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q54',\n",
              "  'question': \"What is the metallicity of Kapteyn's star estimated to be?\",\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q55',\n",
              "  'question': 'What is the SI base unit of time and how is it defined?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q56',\n",
              "  'question': 'What is a planetary system?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q57',\n",
              "  'question': 'What is the result of the collapse of a cavitation bubble?',\n",
              "  'predicted_options': ['C', 'A', 'D']},\n",
              " {'id': 'Q58',\n",
              "  'question': 'Who was Giordano Bruno?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q59',\n",
              "  'question': 'What are the Navier-Stokes equations?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q60',\n",
              "  'question': \"What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?\",\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q61',\n",
              "  'question': 'What is the reason that it is nearly impossible to see light emitted at the Lyman-alpha transition wavelength from a star farther than a few hundred light years from Earth?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q62',\n",
              "  'question': 'What is a Schwarzschild black hole?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q63',\n",
              "  'question': 'What is the definition of Atomristor?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q64',\n",
              "  'question': 'Who published the first theory that was able to encompass previously separate field theories to provide a unifying theory of electromagnetism?',\n",
              "  'predicted_options': ['A', 'E', 'D']},\n",
              " {'id': 'Q65',\n",
              "  'question': \"What is the relevant type of coherence for the Young's double-slit interferometer?\",\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q66',\n",
              "  'question': 'What is the Peierls bracket in canonical quantization?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q67',\n",
              "  'question': \"What is the isophotal diameter used for in measuring a galaxy's size?\",\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q68',\n",
              "  'question': \"What is the Maxwell's Demon thought experiment?\",\n",
              "  'predicted_options': ['A', 'D', 'E']},\n",
              " {'id': 'Q69',\n",
              "  'question': 'What is the application of Memristor?',\n",
              "  'predicted_options': ['D', 'E', 'C']},\n",
              " {'id': 'Q70',\n",
              "  'question': 'What is the effect generated by a spinning superconductor?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q71',\n",
              "  'question': 'What is the main focus of cryogenic and noble liquid detectors in dark matter experiments?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q72',\n",
              "  'question': 'What is a pycnometer?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q73',\n",
              "  'question': 'What is the estimated redshift of CEERS-93316, a candidate high-redshift galaxy observed by the James Webb Space Telescope?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q74',\n",
              "  'question': 'What is bollard pull primarily used for measuring?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q75',\n",
              "  'question': 'What is the piezoelectric strain coefficient for AT-cut quartz crystals?',\n",
              "  'predicted_options': ['B', 'C', 'E']},\n",
              " {'id': 'Q76',\n",
              "  'question': 'What is the difference between probability mass function (PMF) and probability density function (PDF)?',\n",
              "  'predicted_options': ['D', 'A', 'B']},\n",
              " {'id': 'Q77',\n",
              "  'question': 'How do the Lunar Laser Ranging Experiment, radar astronomy, and the Deep Space Network determine distances to the Moon, planets, and spacecraft?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q78',\n",
              "  'question': 'What is the Ozma Problem?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q79',\n",
              "  'question': 'What is a Hilbert space in quantum mechanics?',\n",
              "  'predicted_options': ['E', 'D', 'A']},\n",
              " {'id': 'Q80',\n",
              "  'question': 'What is the significance of the speed of light in vacuum?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q81',\n",
              "  'question': 'What is the term used to describe the proportionality factor to the Stefan-Boltzmann law that is utilized in subsequent evaluations of the radiative behavior of grey bodies?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q82',\n",
              "  'question': 'What is the reason for the formation of stars exclusively within molecular clouds?',\n",
              "  'predicted_options': ['B', 'E', 'D']},\n",
              " {'id': 'Q83',\n",
              "  'question': 'What is the identity operation in symmetry groups?',\n",
              "  'predicted_options': ['A', 'C', 'E']},\n",
              " {'id': 'Q84',\n",
              "  'question': 'What is a regular polytope?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q85',\n",
              "  'question': 'What is the reason behind the largest externally observed electrical effects when two conductors are separated by the smallest distance without touching?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q86',\n",
              "  'question': 'What is the formalism that angular momentum is associated with in rotational invariance?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q87',\n",
              "  'question': 'Which hand should be used to apply the right-hand rule when tightening or loosening nuts, screws, bolts, bottle caps, and jar lids?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q88',\n",
              "  'question': 'What is the Minkowski diagram used for?',\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q89',\n",
              "  'question': 'What are the two main interpretations for the disparity between the presence of matter and antimatter in the observable universe?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q90',\n",
              "  'question': 'What is the Ramsauer-Townsend effect?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q91',\n",
              "  'question': 'What is Minkowski space?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q92',\n",
              "  'question': 'What is the Optical Signal-to-Noise Ratio (OSNR)?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q93',\n",
              "  'question': 'What is the interpretation of supersymmetry in stochastic supersymmetric theory?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q94',\n",
              "  'question': \"What is the purpose of expressing a map's scale as a ratio, such as 1:10,000?\",\n",
              "  'predicted_options': ['E', 'D', 'A']},\n",
              " {'id': 'Q95',\n",
              "  'question': 'What is the main sequence in astronomy?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q96',\n",
              "  'question': 'Who proposed the concept of \"maximal acceleration\"?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q97',\n",
              "  'question': 'What is indirect photophoresis?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q98',\n",
              "  'question': \"What does Earnshaw's theorem state?\",\n",
              "  'predicted_options': ['D', 'E', 'C']},\n",
              " {'id': 'Q99',\n",
              "  'question': 'What is radiosity in radiometry?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q100',\n",
              "  'question': 'What is a virtual particle?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q101',\n",
              "  'question': 'Who proposed the principle of \"complexity from noise\" and when was it first introduced?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q102',\n",
              "  'question': 'What is the order parameter that breaks the electromagnetic gauge symmetry in superconductors?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q103',\n",
              "  'question': 'What is the reason for the sun appearing slightly yellowish when viewed from Earth?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q104',\n",
              "  'question': 'What is the Landau-Lifshitz-Gilbert equation used for in physics?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q105',\n",
              "  'question': 'What is spatial dispersion?',\n",
              "  'predicted_options': ['D', 'A', 'B']},\n",
              " {'id': 'Q106',\n",
              "  'question': 'What are the constituents of cold dark matter?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q107',\n",
              "  'question': 'What is the mechanism of FTIR?',\n",
              "  'predicted_options': ['D', 'E', 'C']},\n",
              " {'id': 'Q108',\n",
              "  'question': 'What is the origin of the permanent moment in paramagnetism?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q109',\n",
              "  'question': \"What is the reason that Newton's second law cannot be used to calculate the development of a physical system in quantum mechanics?\",\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q110',\n",
              "  'question': 'What is the butterfly effect, as defined by Lorenz in his book \"The Essence of Chaos\"?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q111',\n",
              "  'question': 'What is the role of CYCLOIDEA genes in the evolution of bilateral symmetry?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q112',\n",
              "  'question': 'What is the required excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe?',\n",
              "  'predicted_options': ['A', 'D', 'E']},\n",
              " {'id': 'Q113',\n",
              "  'question': 'What is the meaning of the term \"horror vacui\"?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q114',\n",
              "  'question': 'What is the Droste effect?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q115',\n",
              "  'question': 'What is water hammer?',\n",
              "  'predicted_options': ['E', 'C', 'B']},\n",
              " {'id': 'Q116',\n",
              "  'question': 'What is the reason for the stochastic nature of all observed resistance-switching processes?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q117',\n",
              "  'question': 'What is the Einstein@Home project?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q118',\n",
              "  'question': 'What happens to an initially inhomogeneous physical system that is isolated by a thermodynamic operation?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q119',\n",
              "  'question': \"What is the concept of simultaneity in Einstein's book, Relativity?\",\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q120',\n",
              "  'question': 'What is the Josephson effect?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q121',\n",
              "  'question': 'What is the SI unit of the physical quantity m/Q?',\n",
              "  'predicted_options': ['C', 'E', 'D']},\n",
              " {'id': 'Q122',\n",
              "  'question': 'How many crystallographic point groups are there in three-dimensional space?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q123',\n",
              "  'question': 'What is the Liouville density?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q124',\n",
              "  'question': 'What are the four qualitative levels of crystallinity described by geologists?',\n",
              "  'predicted_options': ['B', 'C', 'A']},\n",
              " {'id': 'Q125',\n",
              "  'question': 'What is an order parameter?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q126',\n",
              "  'question': 'What is the significance of the discovery of the Crab pulsar?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q127',\n",
              "  'question': 'What is the De Haas-Van Alphen effect?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q128',\n",
              "  'question': 'What is a \"coffee ring\" in physics?',\n",
              "  'predicted_options': ['E', 'B', 'D']},\n",
              " {'id': 'Q129',\n",
              "  'question': 'What is the significance of probability amplitudes in quantum mechanics?',\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q130',\n",
              "  'question': 'What is the relationship between the amplitude of a sound wave and its loudness?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q131',\n",
              "  'question': 'What are coherent turbulent structures?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q132',\n",
              "  'question': 'What is the main factor that determines the occurrence of each type of supernova?',\n",
              "  'predicted_options': ['E', 'D', 'A']},\n",
              " {'id': 'Q133',\n",
              "  'question': 'What is the Erlangen program?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q134',\n",
              "  'question': 'What is emissivity?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q135',\n",
              "  'question': 'Who was the first person to describe the pulmonary circulation system?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q136',\n",
              "  'question': 'What is the fate of a carbocation formed in crystalline naphthalene?',\n",
              "  'predicted_options': ['E', 'A', 'B']},\n",
              " {'id': 'Q137',\n",
              "  'question': 'What is the main focus of the Environmental Science Center at Qatar University?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q138',\n",
              "  'question': 'What is the purpose of obtaining surgical resection specimens?',\n",
              "  'predicted_options': ['A', 'D', 'E']},\n",
              " {'id': 'Q139',\n",
              "  'question': 'What is the function of mammary glands in mammals?',\n",
              "  'predicted_options': ['A', 'E', 'D']},\n",
              " {'id': 'Q140',\n",
              "  'question': 'What is the relationship between interstellar and cometary chemistry?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q141',\n",
              "  'question': 'What is the reason for recycling rare metals according to the United Nations?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q142',\n",
              "  'question': 'What is radiometric dating?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q143',\n",
              "  'question': 'What is the role of methane in Fischer-Tropsch processes?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q144',\n",
              "  'question': 'What is a phageome?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q145',\n",
              "  'question': 'What is organography?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q146',\n",
              "  'question': 'What is the definition of anatomy?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q147',\n",
              "  'question': 'What is a trophic level in an ecological pyramid?',\n",
              "  'predicted_options': ['C', 'A', 'D']},\n",
              " {'id': 'Q148',\n",
              "  'question': 'What is a crossover experiment?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q149',\n",
              "  'question': 'What is the role of IL-10 in the formation of Tr1 cells and tolerogenic DCs?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q150',\n",
              "  'question': 'What is the reason behind the designation of Class L dwarfs, and what is their color and composition?',\n",
              "  'predicted_options': ['B', 'C', 'A']},\n",
              " {'id': 'Q151',\n",
              "  'question': \"What was Isaac Newton's explanation for rectilinear propagation of light?\",\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q152',\n",
              "  'question': 'What is the relationship between chemical potential and quarks/antiquarks?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q153',\n",
              "  'question': 'What is the American Petroleum Institute (API) gravity?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q154',\n",
              "  'question': 'What are the two main factors that cause resistance in a metal?',\n",
              "  'predicted_options': ['B', 'D', 'E']},\n",
              " {'id': 'Q155',\n",
              "  'question': 'What is the significance of the redshift-distance relationship in determining the expansion history of the universe?',\n",
              "  'predicted_options': ['E', 'C', 'A']},\n",
              " {'id': 'Q156',\n",
              "  'question': 'What is the Evans balance?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q157',\n",
              "  'question': 'What is the definition of dimension in mathematics?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q158',\n",
              "  'question': 'What is accelerator-based light-ion fusion?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q159',\n",
              "  'question': 'What is the interstellar medium (ISM)?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q160',\n",
              "  'question': 'What is the significance of the change in slope of the pinched hysteresis curves in ReRAM and other forms of two-terminal resistance memory?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q161',\n",
              "  'question': 'What is geometric quantization in mathematical physics?',\n",
              "  'predicted_options': ['C', 'D', 'A']},\n",
              " {'id': 'Q162',\n",
              "  'question': 'What is the definition of an improper rotation?',\n",
              "  'predicted_options': ['A', 'B', 'E']},\n",
              " {'id': 'Q163',\n",
              "  'question': 'What is power density in the context of energy systems, and how does it differ between renewable and non-renewable energy sources?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q164',\n",
              "  'question': 'What is Modified Newtonian Dynamics (MOND)?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q165',\n",
              "  'question': 'What is linear frame dragging?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q166',\n",
              "  'question': 'What is explicit symmetry breaking in theoretical physics?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q167',\n",
              "  'question': 'What is the role of the Higgs boson in the Standard Model?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q168',\n",
              "  'question': 'What is Lorentz symmetry or Lorentz invariance in relativistic physics?',\n",
              "  'predicted_options': ['E', 'A', 'C']},\n",
              " {'id': 'Q169',\n",
              "  'question': 'What is the significance of Baryon Acoustic Oscillations (BAOs) in the study of the universe?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q170',\n",
              "  'question': 'What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?',\n",
              "  'predicted_options': ['B', 'C', 'E']},\n",
              " {'id': 'Q171',\n",
              "  'question': 'What are permutation-inversion groups?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q172',\n",
              "  'question': 'What is the relationship between dielectric loss and the transparency of a material?',\n",
              "  'predicted_options': ['B', 'E', 'A']},\n",
              " {'id': 'Q173',\n",
              "  'question': 'What is the purpose of measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs) in ultra-low field MRI?',\n",
              "  'predicted_options': ['C', 'A', 'B']},\n",
              " {'id': 'Q174',\n",
              "  'question': 'What is the difference between illuminance and luminance?',\n",
              "  'predicted_options': ['B', 'C', 'E']},\n",
              " {'id': 'Q175',\n",
              "  'question': 'What is a magnetic monopole in particle physics?',\n",
              "  'predicted_options': ['E', 'C', 'B']},\n",
              " {'id': 'Q176',\n",
              "  'question': 'What is the difference between redshift due to the expansion of the universe and Doppler redshift?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q177',\n",
              "  'question': 'What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?',\n",
              "  'predicted_options': ['D', 'A', 'C']},\n",
              " {'id': 'Q178',\n",
              "  'question': 'What is the reason for heating metals to a temperature just above the upper critical temperature?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q179',\n",
              "  'question': 'What is the cause of the observed change in the periods of moons orbiting a distant planet when measured from Earth?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q180',\n",
              "  'question': 'What is the origin of the radio emission observed from supernova remnants?',\n",
              "  'predicted_options': ['B', 'A', 'E']},\n",
              " {'id': 'Q181',\n",
              "  'question': 'What is the relationship between the Hamiltonians and eigenstates in supersymmetric quantum mechanics?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q182',\n",
              "  'question': 'What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?',\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q183',\n",
              "  'question': 'Which of the following statements accurately describes the characteristics of gravitational waves?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q184',\n",
              "  'question': 'What is the difference between the coevolution of myrmecophytes and the mutualistic symbiosis of mycorrhiza?',\n",
              "  'predicted_options': ['A', 'E', 'B']},\n",
              " {'id': 'Q185',\n",
              "  'question': \"What is the Kelvin-Helmholtz instability and how does it affect Earth's magnetosphere?\",\n",
              "  'predicted_options': ['A', 'E', 'C']},\n",
              " {'id': 'Q186',\n",
              "  'question': 'What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q187',\n",
              "  'question': 'What is the explanation for the effective supersymmetry in quark-diquark models?',\n",
              "  'predicted_options': ['A', 'D', 'E']},\n",
              " {'id': 'Q188',\n",
              "  'question': 'What is the relationship between the complete electromagnetic Hamiltonian of a molecule and the parity operation?',\n",
              "  'predicted_options': ['D', 'C', 'A']},\n",
              " {'id': 'Q189',\n",
              "  'question': 'What is the difference between active and passive transport in cells?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q190',\n",
              "  'question': 'What is the Heisenberg uncertainty principle and how does it relate to angular momentum in quantum mechanics?',\n",
              "  'predicted_options': ['D', 'A', 'E']},\n",
              " {'id': 'Q191',\n",
              "  'question': 'What is the difference between natural convection and forced convection?',\n",
              "  'predicted_options': ['C', 'A', 'E']},\n",
              " {'id': 'Q192',\n",
              "  'question': 'What is magnetic susceptibility?',\n",
              "  'predicted_options': ['A', 'B', 'C']},\n",
              " {'id': 'Q193',\n",
              "  'question': 'What is a transient condensation cloud, also known as a Wilson cloud?',\n",
              "  'predicted_options': ['B', 'E', 'D']},\n",
              " {'id': 'Q194',\n",
              "  'question': 'What is a uniform tiling in the hyperbolic plane?',\n",
              "  'predicted_options': ['D', 'E', 'A']},\n",
              " {'id': 'Q195',\n",
              "  'question': 'What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?',\n",
              "  'predicted_options': ['C', 'E', 'A']},\n",
              " {'id': 'Q196',\n",
              "  'question': 'What is the throttling process, and why is it important?',\n",
              "  'predicted_options': ['B', 'E', 'C']},\n",
              " {'id': 'Q197',\n",
              "  'question': 'What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?',\n",
              "  'predicted_options': ['B', 'A', 'C']},\n",
              " {'id': 'Q198',\n",
              "  'question': \"What is the relationship between mass, force, and acceleration, according to Sir Isaac Newton's laws of motion?\",\n",
              "  'predicted_options': ['D', 'A', 'C']},\n",
              " {'id': 'Q199',\n",
              "  'question': \"What did Arthur Eddington discover about two of Einstein's types of gravitational waves?\",\n",
              "  'predicted_options': ['C', 'E', 'A']}]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_new=calculate_accuracy(inference_results_trained, data)\n",
        "print(f\"Accuracy: {accuracy_new}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rteNIeif0BI5",
        "outputId": "b4bb6198-0f14-440f-ea1a-4fda6ecf6bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAP3_new=calculate_map_at_k(inference_results_trained, data, k=3)\n",
        "print(f\"MAP@3: {MAP3_new}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8JBW7IO9-CT",
        "outputId": "11178e1d-c07e-49f8-c752-71881b2f481f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@3: 0.9541666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_dict = {}\n",
        "data_moved_dict = {}\n",
        "\n",
        "option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "Acc_list = []\n",
        "\n",
        "for target_option_label in option_labels:\n",
        "    # Transform data\n",
        "    transformed_data = transform_data(data, target_option_label)\n",
        "    data_moved_dict[target_option_label] = transformed_data\n",
        "\n",
        "    # Run inference\n",
        "    results = make_inference_on_mcqa_mcp_generate(model, tokenizer, transformed_data,1)\n",
        "    inference_results_dict[target_option_label] = results\n",
        "\n",
        "    # Compute accuracy\n",
        "    Acc_i = calculate_accuracy(results, transformed_data)\n",
        "    Acc_list.append(Acc_i)\n",
        "    print(f\"Accuracy when correct answers are moved to '{target_option_label}': {Acc_i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G59lpfPC-UzW",
        "outputId": "6d77fe85-d543-4654-f501-9ed0d4010ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when correct answers are moved to 'A': 0.835\n",
            "Accuracy when correct answers are moved to 'B': 0.78\n",
            "Accuracy when correct answers are moved to 'C': 0.83\n",
            "Accuracy when correct answers are moved to 'D': 0.85\n",
            "Accuracy when correct answers are moved to 'E': 0.615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate μ_bias\n",
        "Acc_0=accuracy_new\n",
        "Acc_diffs = [abs(Acc_i - Acc_0) for Acc_i in Acc_list]\n",
        "mu_bias = sum(Acc_diffs) / len(option_labels)\n",
        "print(f\"μ_bias: {mu_bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mjD0uImw3m1",
        "outputId": "f737f8e4-ed21-498e-f745-c45a1371775b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "μ_bias: 0.14300000000000007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Point wise Intelligent Feedback Training"
      ],
      "metadata": {
        "id": "e5PcbgdK8KKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_dataset(data, tokenizer, max_length=2048):\n",
        "\n",
        "    transformed_samples = []\n",
        "\n",
        "    options = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        prompt = row['prompt']\n",
        "        option_contents = [row[opt] for opt in options]\n",
        "        answer = row['answer'].strip()\n",
        "\n",
        "        # Construct the input by combining prompt and all options\n",
        "        options_text = \"\"\n",
        "        for opt_symbol, opt_content in zip(options, option_contents):\n",
        "            options_text += f\"{opt_symbol}: {opt_content}\\n\"\n",
        "        input_text = prompt + \"\\n\" + options_text.strip()\n",
        "\n",
        "        # Find the index of the correct answer\n",
        "        correct_option_index = options.index(answer)\n",
        "        correct_option_symbol = options[correct_option_index]\n",
        "        correct_option_content = option_contents[correct_option_index]\n",
        "\n",
        "        # Collect incorrect option symbols and contents\n",
        "        incorrect_symbols = [options[idx] for idx in range(len(options)) if idx != correct_option_index]\n",
        "        incorrect_contents = [option_contents[idx] for idx in range(len(options)) if idx != correct_option_index]\n",
        "\n",
        "        # Randomly select two incorrect symbols\n",
        "        num_negatives = min(2, len(incorrect_symbols))  # Handle cases with fewer than 2 wrong options\n",
        "        selected_symbols = random.sample(incorrect_symbols, num_negatives)\n",
        "\n",
        "        # Randomly select two incorrect contents\n",
        "        selected_contents = random.sample(incorrect_contents, num_negatives)\n",
        "\n",
        "        # Pair the selected symbols with randomly selected contents\n",
        "        negative_samples = []\n",
        "        for sym, cont in zip(selected_symbols, selected_contents):\n",
        "            negative_samples.append(f\"[Negative] {sym}: {cont}\")\n",
        "\n",
        "        # Create the output including positive and negative answers with labels\n",
        "        output_text = f\"[Positive] {correct_option_symbol}: {correct_option_content}\\n\"\n",
        "        output_text += \"\\n\".join(negative_samples)\n",
        "\n",
        "        transformed_samples.append({\n",
        "            'input': input_text,\n",
        "            'output': output_text\n",
        "        })\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = MCQADataset(transformed_samples, tokenizer, max_length)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "GyCEKrSLQN7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_dataset(data, tokenizer, max_length=2048):\n",
        "\n",
        "    transformed_samples = []\n",
        "\n",
        "    options = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        prompt = row['prompt']\n",
        "        option_contents = [row[opt] for opt in options]\n",
        "        answer = row['answer'].strip()\n",
        "\n",
        "        # Construct the input by combining prompt and all options\n",
        "        options_text = \"\"\n",
        "        for opt_symbol, opt_content in zip(options, option_contents):\n",
        "            options_text += f\"{opt_symbol}: {opt_content}\\n\"\n",
        "        input_text = prompt + \"\\n\" + options_text.strip()\n",
        "\n",
        "        # Find the index of the correct answer\n",
        "        correct_option_index = options.index(answer)\n",
        "        correct_option_symbol = options[correct_option_index]\n",
        "        correct_option_content = option_contents[correct_option_index]\n",
        "\n",
        "        # Randomly select an incorrect option\n",
        "        wrong_indices = [i for i in range(len(options)) if i != correct_option_index]\n",
        "        wrong_option_index = random.choice(wrong_indices)\n",
        "        wrong_option_symbol = options[wrong_option_index]\n",
        "        wrong_option_content = option_contents[wrong_option_index]\n",
        "\n",
        "        # Create the output including both positive and negative answers with labels\n",
        "        output_text = (\n",
        "            f\"[Positive] {correct_option_symbol}: {correct_option_content}\\n\"\n",
        "            f\"[Negative] {wrong_option_symbol}: {wrong_option_content}\"\n",
        "        )\n",
        "\n",
        "        transformed_samples.append({\n",
        "            'input': input_text,\n",
        "            'output': output_text\n",
        "        })\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = MCQADataset(transformed_samples, tokenizer, max_length)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "dtLFSCJtQpuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Custom Data Collator\n",
        "def data_collator(features):\n",
        "    batch = {}\n",
        "    input_ids = [f['input_ids'] for f in features]\n",
        "    attention_mask = [f['attention_mask'] for f in features]\n",
        "    labels = [f['labels'] for f in features]\n",
        "    positive_masks = [f['positive_mask'] for f in features]\n",
        "    negative_masks = [f['negative_mask'] for f in features]\n",
        "\n",
        "    # Pad sequences\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
        "        attention_mask, batch_first=True, padding_value=0\n",
        "    )\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100\n",
        "    )\n",
        "    positive_masks = torch.nn.utils.rnn.pad_sequence(\n",
        "        positive_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "    negative_masks = torch.nn.utils.rnn.pad_sequence(\n",
        "        negative_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "\n",
        "    batch['input_ids'] = input_ids\n",
        "    batch['attention_mask'] = attention_mask\n",
        "    batch['labels'] = labels\n",
        "    batch['positive_mask'] = positive_masks\n",
        "    batch['negative_mask'] = negative_masks\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "V-7ctIF0Qqy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PIFTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop('labels')\n",
        "        positive_mask = inputs.pop('positive_mask').to(model.device)\n",
        "        negative_mask = inputs.pop('negative_mask').to(model.device)\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Shift tokens for next-token prediction\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        positive_mask = positive_mask[..., 1:].contiguous()\n",
        "        negative_mask = negative_mask[..., 1:].contiguous()\n",
        "\n",
        "        # Flatten the tokens\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=-100)\n",
        "        loss = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1)\n",
        "        )\n",
        "        loss = loss.view(shift_labels.size())\n",
        "\n",
        "        # Calculate losses for positive and negative samples\n",
        "        positive_loss = (loss * positive_mask).sum(dim=1) / (positive_mask.sum(dim=1) + 1e-8)\n",
        "        negative_loss = (loss * negative_mask).sum(dim=1) / (negative_mask.sum(dim=1) + 1e-8)\n",
        "\n",
        "        # PIF loss calculation\n",
        "        lambda_neg = 0.01  # λ for negative samples\n",
        "        log_lambda_neg = torch.log(torch.tensor(lambda_neg, device=model.device))\n",
        "\n",
        "        # For positive samples\n",
        "        positive_pif_loss = positive_loss  # We aim to minimize this loss\n",
        "\n",
        "        # For negative samples\n",
        "        zero_tensor = torch.zeros_like(negative_loss)\n",
        "        negative_pif_loss = torch.max(zero_tensor, negative_loss - log_lambda_neg)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = positive_pif_loss + negative_pif_loss\n",
        "        loss = total_loss.mean()\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, outputs\n",
        "        else:\n",
        "            return loss"
      ],
      "metadata": {
        "id": "CNzpr8_HC1ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCQADataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, max_length=1024):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, int):\n",
        "            return self.process_sample(idx)\n",
        "        elif isinstance(idx, list):\n",
        "            batch = {'input_ids': [], 'attention_mask': [], 'labels': [], 'positive_mask': [], 'negative_mask': []}\n",
        "            for i in idx:\n",
        "                sample = self.process_sample(i)\n",
        "                for key in batch:\n",
        "                    batch[key].append(sample[key])\n",
        "            return batch\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported index type: {type(idx)}\")\n",
        "\n",
        "    def process_sample(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        input_text = sample['input']\n",
        "        output_text = sample['output']\n",
        "\n",
        "        # Concatenate input and output for causal language modeling\n",
        "        full_text = input_text + \"\\nAnswer:\\n\" + output_text\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # Find the length of the input prompt plus \"Answer:\\n\"\n",
        "        input_prefix = input_text + \"\\nAnswer:\\n\"\n",
        "        input_prefix_encoding = self.tokenizer(\n",
        "            input_prefix,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_prefix_length = input_prefix_encoding['input_ids'].size(1)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        # Mask the input_text tokens and \"Answer:\\n\"\n",
        "        labels[:input_prefix_length] = -100  # Ignore index for loss computation\n",
        "\n",
        "        # Create masks for positive and negative samples\n",
        "        positive_mask, negative_mask = self.create_masks(input_ids, input_prefix_length)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "            'positive_mask': positive_mask,\n",
        "            'negative_mask': negative_mask\n",
        "        }\n",
        "\n",
        "    def create_masks(self, input_ids, input_prefix_length):\n",
        "        labels = input_ids\n",
        "        positive_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "        negative_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "\n",
        "        # Token IDs for [Positive] and [Negative]\n",
        "        positive_token_ids = self.tokenizer.encode(\"[Positive]\", add_special_tokens=False)\n",
        "        negative_token_ids = self.tokenizer.encode(\"[Negative]\", add_special_tokens=False)\n",
        "\n",
        "        # Identify positions of [Positive] and [Negative] in input_ids\n",
        "        i = input_prefix_length\n",
        "        while i < len(labels):\n",
        "            if input_ids[i:i+len(positive_token_ids)].tolist() == positive_token_ids:\n",
        "                # Mark tokens following [Positive]\n",
        "                j = i + len(positive_token_ids)\n",
        "                while j < len(labels):\n",
        "                    if input_ids[j:j+len(negative_token_ids)].tolist() == negative_token_ids:\n",
        "                        break\n",
        "                    positive_mask[j] = True\n",
        "                    j += 1\n",
        "                i = j\n",
        "            elif input_ids[i:i+len(negative_token_ids)].tolist() == negative_token_ids:\n",
        "                # Mark tokens following [Negative]\n",
        "                j = i + len(negative_token_ids)\n",
        "                while j < len(labels):\n",
        "                    if input_ids[j:j+len(positive_token_ids)].tolist() == positive_token_ids:\n",
        "                        break\n",
        "                    negative_mask[j] = True\n",
        "                    j += 1\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        return positive_mask, negative_mask"
      ],
      "metadata": {
        "id": "QVIIVHB-Ur0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = ['[Positive]', '[Negative]']\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "tokenizer.add_tokens(special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gNvH_mh8YPR",
        "outputId": "de39fe0f-9fb3-4b57-cc0c-df0d250af526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gX3Yodz8c9g",
        "outputId": "978ebb02-1900-469a-fc91-048848a4f043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(128258, 3072, padding_idx=128004)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = transform_dataset(train, tokenizer, max_length=2048)"
      ],
      "metadata": {
        "id": "4vfu4p0TQ84Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0  # Change this index to view different samples\n",
        "sample = dataset[sample_idx]\n",
        "print(\"Input Text:\\n\", dataset.samples[sample_idx]['input'])\n",
        "print(\"\\nOutput Text:\\n\", dataset.samples[sample_idx]['output'])\n",
        "print(\"\\nPositive Mask:\", sample['positive_mask'])\n",
        "print(\"\\nNegative Mask:\", sample['negative_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3GG4afuRgE8",
        "outputId": "371e7133-e9f8-410d-f534-bb1bd159fd7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:\n",
            " Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
            "A: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
            "B: MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
            "C: MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
            "D: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
            "E: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
            "\n",
            "Output Text:\n",
            " [Positive] D: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
            "[Negative] A: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
            "\n",
            "Positive Mask: tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False])\n",
            "\n",
            "Negative Mask: tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        remove_unused_columns=False,# Use this for WandB etc\n",
        "    )"
      ],
      "metadata": {
        "id": "FjqMBfnZTFJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = PIFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO7CqS-iSOfV",
        "outputId": "3ad460d7-0535-4e7d-b035-a6deee5d7f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ],
      "metadata": {
        "id": "L4hHgZrXVxij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eEyTFz7aTNfG",
        "outputId": "b39f39da-9cc5-490d-e826-3fe3634b7c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 100\n",
            " \"-____-\"     Number of trainable parameters = 2,818,747,392\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:48, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>25.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>23.967500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>23.405200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>26.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>20.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>21.212500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>20.805400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>20.118800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>21.355000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>19.958100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>20.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>21.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>19.967000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>21.839700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>20.113600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>19.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>20.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>19.877400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>20.197200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>19.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>19.534700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>20.341300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>20.028500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>20.371200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>20.308600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>19.512400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>19.747400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>19.898400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>19.882600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>19.989600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>20.087500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>19.761500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>19.517800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>19.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>20.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>19.493800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>20.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>19.716600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>19.565900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>20.939200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>20.381200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>20.384200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>20.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>20.199200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>19.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>20.007200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>19.984900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>20.308500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>20.135600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>21.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>20.168800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>19.885500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>20.042800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>20.736900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>20.228800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>19.531900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>20.594100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>20.039200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>19.682900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>19.363300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>19.230600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>19.781800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>19.953000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>19.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>19.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>19.472300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>19.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>19.824200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>20.947800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>19.697100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>20.799500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>19.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>19.536800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>19.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>19.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>19.839500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>19.785600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>19.547000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>20.160600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>19.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>20.267000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>19.667000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>19.420400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>19.338300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>19.477200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>20.176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>20.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>19.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>20.064300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>19.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>19.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>19.259000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>19.523200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>19.677700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>19.403400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>19.746800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>21.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>19.736900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>19.973000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>19.647400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=20.183938446044923, metrics={'train_runtime': 115.1851, 'train_samples_per_second': 6.945, 'train_steps_per_second': 0.868, 'total_flos': 5197409391181824.0, 'train_loss': 20.183938446044923, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = '/content/drive/MyDrive/models/PIF_LORA_llama7'\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sxBXWsxWSYD",
        "outputId": "ee950d01-1e55-475c-a542-42e9ec4e667b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/models/PIF_LORA_llama7/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/models/PIF_LORA_llama7/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/models/PIF_LORA_llama7/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/models/PIF_LORA_llama7\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "325a09a041ea44029a3368d3492b9cc5",
            "bc69dc80d16d4b12b83a7cadf9e98050",
            "43d7e807b14d445f9a83089189931eb6",
            "78e54325eec64a61b7f67786727aef60",
            "4a23cfc458ca48ab97bbf71d98d236a5",
            "26fda3197a294f9b9d4ffc2c704abe17",
            "5706070ac9bc4c17beec02cb5fcd647f",
            "01bd7fec617342f5a4516805cede0b9e",
            "0816e88c969d42c98129dcb60495cc01",
            "23fe201e989342479382a352e026a19f",
            "37e6bd0e5d964e1da98ac7583246b974"
          ]
        },
        "id": "qQzEOmyDWlQb",
        "outputId": "7442f67d-b188-4241-da3a-8191b3451d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "325a09a041ea44029a3368d3492b9cc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128258, 3072, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=128258, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_trained2 = make_inference_on_mcqa_mcp_generate(model,tokenizer,data,3)"
      ],
      "metadata": {
        "id": "zWv5n2jyXLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_new2=calculate_accuracy(inference_results_trained2, data)\n",
        "print(f\"Accuracy: {accuracy_new2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opbcgfLyW0h2",
        "outputId": "6d0fd667-02d4-43e6-acfa-40278cda7b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_results_dict = {}\n",
        "data_moved_dict = {}\n",
        "\n",
        "option_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "Acc_list = []\n",
        "\n",
        "for target_option_label in option_labels:\n",
        "    # Transform data\n",
        "    transformed_data = transform_data(data, target_option_label)\n",
        "    data_moved_dict[target_option_label] = transformed_data\n",
        "\n",
        "    # Run inference\n",
        "    results = make_inference_on_mcqa_mcp_generate(model, tokenizer, transformed_data,1)\n",
        "    inference_results_dict[target_option_label] = results\n",
        "\n",
        "    # Compute accuracy\n",
        "    Acc_i = calculate_accuracy(results, transformed_data)\n",
        "    Acc_list.append(Acc_i)\n",
        "    print(f\"Accuracy when correct answers are moved to '{target_option_label}': {Acc_i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVMxkpD3XTA6",
        "outputId": "ca3fba2e-cda3-4bfb-f9a1-6db32c97f1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when correct answers are moved to 'A': 0.705\n",
            "Accuracy when correct answers are moved to 'B': 0.93\n",
            "Accuracy when correct answers are moved to 'C': 0.865\n",
            "Accuracy when correct answers are moved to 'D': 0.85\n",
            "Accuracy when correct answers are moved to 'E': 0.585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAP3_new2=calculate_map_at_k(inference_results_trained2, data, k=3)\n",
        "print(f\"MAP@3: {MAP3_new2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIGgdxWTE6pe",
        "outputId": "ccbe2808-999c-40b6-f4ab-d570e7e2bae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@3: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate μ_bias\n",
        "Acc_0=accuracy_new2\n",
        "Acc_diffs = [abs(Acc_i - Acc_0) for Acc_i in Acc_list]\n",
        "mu_bias = sum(Acc_diffs) / len(option_labels)\n",
        "print(f\"μ_bias: {mu_bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itjWmOC67B1k",
        "outputId": "70126c16-3855-47a4-8fec-8315d5d4ae8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "μ_bias: 0.14300000000000007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PIF_LORA_llama5 best on accuracy\n",
        "//\n",
        "\n",
        "PIF_LORA_llama7 best on accuracy\n"
      ],
      "metadata": {
        "id": "cUnFkqFCFQ6c"
      }
    }
  ]
}